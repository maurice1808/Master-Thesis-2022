{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import time\n",
    "import clip\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import glob\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim, vocab_size, token_embedding):\n",
    "        super().__init__()\n",
    "        self.sparse_projection = nn.Linear(embed_dim, vocab_size)\n",
    "        self.sparse_projection.weight = nn.Parameter(token_embedding.weight)\n",
    "    \n",
    "    def forward(self, dense_vec):\n",
    "        logits = self.sparse_projection(dense_vec)\n",
    "        lex_weights = torch.log1p(torch.relu(logits))\n",
    "        return lex_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoded_vectors():\n",
    "    def __init__(self, image_vectors, caption_vectors):\n",
    "\n",
    "        self.images = image_vectors\n",
    "        self.captions  = caption_vectors\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        caption = self.captions[idx]\n",
    "        return image,caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sparse_performance(model, test_ims, test_cap):\n",
    "    with torch.no_grad():\n",
    "        x, _ = test_ims.shape\n",
    "        test_ims = test_ims[0::5]\n",
    "        encoded_ims = torch.empty(int(x/5), 49408).to(device)\n",
    "        for i in range(0, len(encoded_ims), 64):\n",
    "            encoded_ims[i:i+64] = model(test_ims[i:i+64])\n",
    "\n",
    "        encoded_caps = torch.empty(int(x), 49408).to(device)\n",
    "        for i in range(0, len(encoded_caps), 64):\n",
    "            encoded_caps[i:i+64] = model(test_cap[i:i+64])   \n",
    " \n",
    "        encoded_ims = (encoded_ims / encoded_ims.norm(dim=-1, keepdim=True)).to(\"cpu\")\n",
    "        encoded_caps = (encoded_caps / encoded_caps.norm(dim=-1, keepdim=True)).to(\"cpu\")\n",
    "        \n",
    "        recall_1, recall_5, recall_10 = [],[],[]\n",
    "        i = 0\n",
    "        j = 0\n",
    "        image_id = 0\n",
    "        print(len(encoded_caps))\n",
    "        t = time.time()\n",
    "        for text_feature in encoded_caps:\n",
    "            if (j%100) == 0:\n",
    "                print(j, \", time since last: \", time.time() - t, end='\\r')\n",
    "                t = time.time()\n",
    "            similarity = (100.0 * text_feature @ encoded_ims.T).softmax(dim=-1)\n",
    "            _, indices = similarity.topk(10)\n",
    "\n",
    "            recall_1.append(image_id in indices[0])\n",
    "            recall_5.append(image_id in indices[:5])\n",
    "            recall_10.append(image_id in indices)\n",
    "    \n",
    "            i += 1\n",
    "            j += 1\n",
    "            if i == 5:\n",
    "                i = 0\n",
    "                image_id += 1\n",
    "\n",
    "        return torch.Tensor(recall_1).mean(), torch.Tensor(recall_5).mean(), torch.Tensor(recall_10).mean()                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sparse_performance(model, test_ims, test_cap):\n",
    "    with torch.no_grad():\n",
    "        x, _ = test_ims.shape\n",
    "        encoded_ims = torch.empty(int(x), 49408).to(device)\n",
    "        for i in range(0, len(encoded_ims), 64):\n",
    "            encoded_ims[i:i+64] = model(test_ims[i:i+64])\n",
    "\n",
    "        encoded_caps = torch.empty(int(x*5), 49408).to(device)\n",
    "        for i in range(0, len(encoded_caps), 64):\n",
    "            encoded_caps[i:i+64] = model(test_cap[i:i+64])   \n",
    " \n",
    "        encoded_ims = encoded_ims / encoded_ims.norm(dim=-1, keepdim=True)\n",
    "        encoded_caps = encoded_caps / encoded_caps.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        recall_1, recall_5, recall_10 = [],[],[]\n",
    "        i = 0\n",
    "        j = 0\n",
    "        image_id = 0\n",
    "        print(len(encoded_caps))\n",
    "        t = time.time()\n",
    "        for text_feature in encoded_caps:\n",
    "            if (j%1000) == 0:\n",
    "                print(j, \", time since last: \", time.time() - t, end='\\r')\n",
    "                t = time.time()\n",
    "            similarity = (100.0 * text_feature @ encoded_ims.T).softmax(dim=-1)\n",
    "            _, indices = similarity.topk(10)\n",
    "\n",
    "            recall_1.append(image_id in indices[0])\n",
    "            recall_5.append(image_id in indices[:5])\n",
    "            recall_10.append(image_id in indices)\n",
    "    \n",
    "            i += 1\n",
    "            j += 1\n",
    "            if i == 5:\n",
    "                i = 0\n",
    "                image_id += 1\n",
    "\n",
    "        return torch.Tensor(recall_1).mean(), torch.Tensor(recall_5).mean(), torch.Tensor(recall_10).mean()                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dense_performance(encoded_ims, encoded_caps):\n",
    "    encoded_ims = encoded_ims[0::5] / encoded_ims[0::5].norm(dim=-1, keepdim=True)\n",
    "    encoded_caps = encoded_caps / encoded_caps.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    recall_1 = []\n",
    "    recall_5 = []\n",
    "    recall_10 = []\n",
    "    i = 0\n",
    "    image_id = 0\n",
    "    for text_feature in encoded_caps:\n",
    "        similarity = (100.0 * text_feature.to(device) @ encoded_ims.T).softmax(dim=-1)\n",
    "\n",
    "        values, indices = similarity.topk(1)\n",
    "        recall_1.append(image_id in indices)\n",
    "\n",
    "        values, indices = similarity.topk(5)\n",
    "        recall_5.append(image_id in indices)\n",
    "\n",
    "        values, indices = similarity.topk(10)\n",
    "        recall_10.append(image_id in indices)\n",
    "\n",
    "        i += 1\n",
    "        if i == 5:\n",
    "            i = 0\n",
    "            image_id += 1\n",
    "\n",
    "    recall_1 = torch.Tensor(recall_1)\n",
    "    recall_5 = torch.Tensor(recall_5)\n",
    "    recall_10 = torch.Tensor(recall_10)\n",
    "    \n",
    "    return recall_1.mean(), recall_5.mean(), recall_10.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"ViT-L/14@336px\" \"ViT-B/32\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359664128\n",
      "101385216\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated())\n",
    "model = 0\n",
    "print(torch.cuda.memory_allocated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\" ,device=device,jit=False)\n",
    "\n",
    "checkpoint = torch.load(\"D:/thesis/Models/flickr30k trained/92.pt\", map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = model.text_projection.shape[1]\n",
    "sparse_mlm = SparseEncoder(embed_dim, model.vocab_size, model.token_embedding).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open the json file for annotations\n",
    "f = open('datasets/flickr30k/dataset_flickr30k.json')\n",
    "data = json.load(f)\n",
    "\n",
    "# load all captions\n",
    "caps = [x['sentences'] for x in data['images'] if x.get('split') == 'train']\n",
    "test_caps = [x['sentences'] for x in data['images'] if x.get('split') == 'test']\n",
    "\n",
    "files = []\n",
    "test_files = []\n",
    "for x in data['images']:\n",
    "    if x.get('split') == 'train':\n",
    "        files.append('datasets/flickr30k/train/' + x['filename'])\n",
    "    \n",
    "    elif x.get('split') == 'test':\n",
    "        test_files.append('datasets/flickr30k/test/' + x['filename'])\n",
    "            \n",
    "captions = []\n",
    "for x in caps:\n",
    "    for y in x:\n",
    "        captions.append(y)\n",
    "        \n",
    "test_captions = []\n",
    "for x in test_caps:\n",
    "    for y in x:\n",
    "        test_captions.append(y)\n",
    "    \n",
    "captions = [x.get('raw') for x in captions]\n",
    "test_captions = [x.get('raw') for x in test_captions]\n",
    "\n",
    "# these captions are too long and need adjusting\n",
    "captions[13035] = 'Four young adults sit outside on a wooden deck near a building around a small round table, while another person stands on the edge of the deck, leaning on the wooden railing, with the sun shining on one of them, one holding a cellphone out in front of himself and another holding a green and red soda can.'\n",
    "captions[14580] = 'A man wearing a helmet, red pants with white and a white and red shirt is on a small bicycle using only his hands, while another man wearing a light blue shirt with dark blue trim and black pants with red stripes is standing nearby, gesturing toward the first man and holding a small figurine.'\n",
    "captions[120165] = 'In this photo there is a man in a dirty white shirt and a black hat with yellow stained teeth, he looks happy and it appears that he is also repairing something.'\n",
    "test_captions[3905] = 'Two boys are looking upwards with their arms streteched to the sky, the boy on the left is wearing a blue vest jacket with a gray shirt, black jogging pants and a hat, and the boy on the right is wearing a silver vest jacket, with blue long-sleeved undershirt, gray pants, black tennis shoes and has black short hair and glasses.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = files[0:10]\n",
    "captions = captions[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28992 / 29000 988980736\n",
      "done\n",
      "1681.2453961372375\n"
     ]
    }
   ],
   "source": [
    "# encode images\n",
    "L = len(files)\n",
    "batch_size = 32\n",
    "t1 = time.time()\n",
    "with torch.no_grad():\n",
    "    encoded_ims = torch.Tensor()\n",
    "    for i in range(0, L, batch_size):\n",
    "        print(i,\"/\",L, torch.cuda.memory_allocated(), end='\\r')\n",
    "        images = torch.Tensor().to(device)\n",
    "        for x in range(batch_size):\n",
    "            if (i + x) < L:\n",
    "                image = preprocess(Image.open(files[i+x])).to(device)\n",
    "                images = torch.cat((images, image.reshape(1,3,336,336)), 0)\n",
    "                \n",
    "        ims = model.encode_image(images)\n",
    "        encoded_ims = torch.cat((encoded_ims, torch.repeat_interleave(ims.to(\"cpu\"), 5, dim=0)), 0)\n",
    "\n",
    "encoded_images = encoded_ims.to(device)\n",
    "print(\"\")\n",
    "print(\"done\")\n",
    "print(time.time() - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vitb32, vitb32Ft, vitL14@336"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_images = torch.load(\"pt datafiles/flickr/encoded_images_vitb32Ft.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([145000, 512])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoded_images, \"encoded_images_vitL14@336.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144896 / 145000\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "text = clip.tokenize(captions).to(device)\n",
    "\n",
    "# encode captions\n",
    "text_features = torch.Tensor().to(device)\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(text), 128):\n",
    "        ts = model.encode_text(text[i:i+128])\n",
    "        text_features = torch.cat((text_features, ts), 0)\n",
    "        if (i%256) == 0:\n",
    "            print(i,\"/\",len(text), end='\\r')\n",
    "\n",
    "print(\"\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features = torch.load(\"pt datafiles/flickr/encoded_captions_vitb32Ft.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([145000, 512])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(text_features, \"encoded_captions_vitL14@336.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "992 / 1000 985397760\n",
      "done\n",
      "6.27539587020874\n"
     ]
    }
   ],
   "source": [
    "# encode images\n",
    "L = len(test_files)\n",
    "batch_size = 32\n",
    "t1 = time.time()\n",
    "with torch.no_grad():\n",
    "    test_ims = torch.Tensor()\n",
    "    for i in range(0, L, batch_size):\n",
    "        print(i,\"/\",L, torch.cuda.memory_allocated(), end='\\r')\n",
    "        images = torch.Tensor().to(device)\n",
    "        for x in range(batch_size):\n",
    "            if (i + x) < L:\n",
    "                image = preprocess(Image.open(test_files[i+x])).to(device)\n",
    "                images = torch.cat((images, image.reshape(1,3,224,224)), 0)\n",
    "                \n",
    "        ims = model.encode_image(images)\n",
    "        test_ims = torch.cat((test_ims, ims.to(\"cpu\")), 0)\n",
    "\n",
    "test_images = test_ims.to(device)\n",
    "print(\"\")\n",
    "print(\"done\")\n",
    "print(time.time() - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = torch.load(\"pt datafiles/flickr/test_images_vitb32Ft.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 512])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(test_images, \"test_images_vitb32Ft.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4864 / 5000\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "text = clip.tokenize(test_captions).to(device)\n",
    "\n",
    "# encode captions\n",
    "test_features = torch.Tensor().to(device)\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(text), 128):\n",
    "        ts = model.encode_text(text[i:i+128])\n",
    "        test_features = torch.cat((test_features, ts), 0)\n",
    "        if (i%256) == 0:\n",
    "            print(i,\"/\",len(text), end='\\r')\n",
    "\n",
    "print(\"\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = torch.load(\"pt datafiles/flickr/test_captions_vitb32Ft.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 512])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(test_features, \"test_captions_vitL14@336.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 64\n",
    "\n",
    "dataset = encoded_vectors(encoded_images, text_features)\n",
    "train_dataloader = DataLoader(dataset, batch_size = batchsize, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "tensor(0.5302)nce last:  0.9143605232238772\n",
      "tensor(0.7908)\n",
      "tensor(0.8626)\n"
     ]
    }
   ],
   "source": [
    "#before training\n",
    "rec1,rec5,rec10 = test_sparse_performance(sparse_mlm, test_images, test_features)\n",
    "print(rec1)\n",
    "print(rec5)\n",
    "print(rec10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 757.94170853123072265\n",
      "5000\n",
      "R@1: tensor(0.5934)ast:  0.9087588787078857\n",
      "\n",
      "loss: 519.247769869864 2265\n",
      "5000\n",
      "R@1: tensor(0.5816)ast:  0.9207675457000732\n",
      "\n",
      "loss: 346.55492580309512265\n",
      "5000\n",
      "R@1: tensor(0.5964)ast:  0.9252562522888184\n",
      "\n",
      "loss: 314.50334586482495265\n",
      "5000\n",
      "R@1: tensor(0.5912)ast:  0.9593751430511475\n",
      "\n",
      "loss: 291.91045474354182265\n",
      "5000\n",
      "R@1: tensor(0.5908)ast:  0.8997485637664795\n",
      "\n",
      "loss: 269.31393012590706265\n",
      "5000\n",
      "R@1: tensor(0.5930)ast:  0.9057459831237793\n",
      "\n",
      "loss: 268.77857599034905265\n",
      "5000\n",
      "R@1: tensor(0.5926)ast:  0.9002456665039062\n",
      "\n",
      "loss: 269.68633797485382265\n",
      "5000\n",
      "R@1: tensor(0.5926)ast:  0.9010930061340332\n",
      "\n",
      "loss: 265.61915476713332265\n",
      "5000\n",
      "R@1: tensor(0.5922)ast:  0.9012835025787354\n",
      "\n",
      "loss: 265.87353944778442265\n",
      "5000\n",
      "R@1: tensor(0.5920)ast:  0.9253442287445068\n",
      "\n",
      "\n",
      "done\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAEICAYAAAB25L6yAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAYjElEQVR4nO3deXCc9X3H8fd3ddmSbMs6gg/5wjEmBmIsVMIVJ4UckBBowO2Q5mhapp60KYHQi7TTc5qmadOUtCHJuATaTlJoYqApDjFJm4OYlEOWDfiAFnxJsqll+UKSkSztt3/soxPZWtlaPdfnNbOj3X2eXX/ZwR89/u3z2TV3R0REoisT9gAiInJ6CmoRkYhTUIuIRJyCWkQk4hTUIiIRp6AWEYk4BbWISMQpqCUWzGyPmb1r1H2lZrY+2OZm9s5wphMpLAW1xN0m4CPAqxN9oJkVT/44IpNPQS2x5e697n63u28C+sfb38zeaWatZvb7ZvYqcL+ZZczsLjN7xcw6zOxbZlY97DEfM7O9wbY/GuvIXqTQFNSSNnOAamARsBa4DfgF4B3APOAIcA+Ama0AvgJ8GJgLzALmT/nEknoFC2ozu8/MDprZtjz3/yUz22Fm283sXws1l6ReFvgTd+9x9xPAJ4A/dPdWd+8B/hRYEyyLrAEedfdN7t4L/DGgD8eRKVfINbp/Ar4M/Mt4O5rZMuAzwJXufsTM3lTAuSQFzGwhsGPgtrtXBlfb3f31YbsuAh4xs+yw+/qBc8gdYbcMe45uM+so3NQiYyvYEbW7PwEcHn6fmS01s41mttnMfmpm5webfh24x92PBI89WKi5JB3cfZ+7Vw5chm8atWsLcJ27Vw27THP3NuAAUD+wo5lNB2oKP73ISFO9Rr0OuM3dLwF+h9z6H8B5wHlm9qSZPWVm107xXBIPJWY2bdil2MzKzGxasL00uN8m8JxfAz5rZosAzKzOzG4Mtq0HPmBmV5hZKbllkYk8t8ikmLLTk8ysErgC+Pawv0dlw+ZYBryT3BHME2Z2kbsfnar5JBYeG3X7s+ROzVsU3H48+LkE2JPnc36JXPh+38zmAQeBfwO+4+7bzew24EGgArg72N5zhvOLnBEr5BcHmNliYIO7X2hmM4GX3H3uGPt9DXja3e8Pbv8XcJe7P1uw4UQmKDjYOAosc/fdIY8jKTJlSx/ufhzYbWa/CGA5K4PN/07uaBozqyW3FLJrqmYTORUz+4CZlZtZBfAF4AXyP1oXmRSFPD3vAeC/geVByeBWcuej3mpmzwHbgYG1wMeBDjPbAfwI+F1317vrEgU3AvuDyzLgFtf318kUK+jSh4iInD01E0VEIq4gZ33U1tb64sWLC/HUIiKJtHnz5kPuXjfWtoIE9eLFi2lqairEU4uIJJKZ7T3VNi19iIhEnIJaRCTiFNQiIhGnoBYRiTgFtYhIxCmoRUQibtygNrPlZrZ12OW4md0xBbOJiAh5nEft7i8BFwOYWRHQBjwy2YP09mW578ndrJg7k9XnjXnOt4hIKk106eMa4BV3P+WJ2WeqpMhY98QuHn1u/2Q/tYhIrE00qG8BHijEIGbGqgVVNO87UoinFxGJrbyDOvgqohuAb59i+1ozazKzpvb29jMapmHRbF5p7+Jod+8ZPV5EJIkmckR9HdDs7v831kZ3X+fuje7eWFd3ZmvMqxZWAbCl5egZPV5EJIkmEtQfokDLHgNW1leRMdiyV8sfIiID8grq4GuI3g08XMhhKsqKWT5nJs37jhbyjxERiZW8gtrdu9y9xt2PFXqghoVVbG05Sn9W3zwjIgIRbCY2LJxNZ08f/3vwtbBHERGJhOgF9aLZADTvPRruICIiERG5oF5cU051RSlbdD61iAgQwaBW8UVEZKTIBTWo+CIiMlwkg1rFFxGRIZEMahVfRESGRDKoVXwRERkSyaAGFV9ERAZEOKhVfBERgSgHdVB82aLlDxFJucgG9UDxpVlvKIpIykU2qFV8ERHJiWxQg4ovIiIQ8aBetaAKUPFFRNIt0kG9coGKLyIikQ5qFV9ERCIe1KDii4hIDIJaxRcRSbfoB7WKLyKScpEPahVfRCTtIh/UKr6ISNpFPqgh90UCKr6ISFrFIqgbFgbr1Cq+iEgKxSKoVXwRkTSLRVCr+CIiaRaLoAYVX0QkvWIU1Lniy8sHO8MeRURkSsUnqIPii07TE5G0iU1Qq/giImkVm6BW8UVE0io2QQ0qvohIOuUV1GZWZWbrzexFM9tpZpcXerCxqPgiImmU7xH1l4CN7n4+sBLYWbiRTk3FFxFJo+LxdjCzWcBq4OMA7t4LhLL2oOKLiKRRPkfUS4B24H4z22Jm95pZxeidzGytmTWZWVN7e/ukDzpAxRcRSZt8groYaAC+6u6rgC7grtE7ufs6d29098a6urpJHnOIii8ikjb5BHUr0OruTwe315ML7lCo+CIiaTNuULv7q0CLmS0P7roG2FHQqU5jcU05s8tLVHwRkdQY983EwG3AN82sFNgF/GrhRjo9M2PVwtk6ohaR1MgrqN19K9BY2FHy17Cwih++eJCj3b1UlZeGPY6ISEHFqpk4QMUXEUmTWAa1ii8ikiaxDGoVX0QkTWIZ1KDii4ikR4yDWsUXEUmH+Aa1ii8ikhKxDWoVX0QkLWIb1Cq+iEhaxDaoIfeGor7xRUSSLuZBreKLiCRfrINaxRcRSYNYB7WKLyKSBrEOalDxRUSSLwFBreKLiCRb/INaxRcRSbjYB7WKLyKSdLEPahVfRCTpYh/UoOKLiCRbQoJaxRcRSa5EBLWKLyKSZIkI6oHii46oRSSJEhHUEBRf9h0lq+KLiCRMgoJ6Nq/19PG/Kr6ISMIkJqhXLawCVHwRkeRJTFAvqa1Q8UVEEikxQa3ii4gkVWKCGlR8EZFkSlhQq/giIsmTqKAeLL7oiwREJEESFdSDxRetU4tIgiQqqEHFFxFJngQGtYovIpIsxfnsZGZ7gNeAfqDP3RsLOdTZGF58WT5nRrjDiIhMgokcUf+8u18c5ZAGFV9EJHkSt/Sh4ouIJE2+Qe3A981ss5mtHWsHM1trZk1m1tTe3j55E54BFV9EJEnyDeqr3L0BuA74pJmtHr2Du69z90Z3b6yrq5vUISdKxRcRSZK8gtrd24KfB4FHgEsLOdTZUvFFRJJk3KA2swozmzFwHXgPsK3Qg50NFV9EJEnyOaI+B9hkZs8BzwDfdfeNhR3r7Kn4IiJJMW5Qu/sud18ZXC5w989OxWBna5WKLyKSEIk7PW9Ag77xRUQSIrFBreKLiCRFYoNaxRcRSYrEBjWo+CIiyZDwoM4VX7aq+CIiMZbooB4ovjSr+CIiMZbooFbxRUSSINFBDSq+iEj8JT6oVXwRkbhLfFCr+CIicZf4oFbxRUTiLvFBreKLiMRd4oMaVHwRkXhLSVCr+CIi8ZWKoFbxRUTiLBVBreKLiMRZKoIaYJWKLyISU6kJ6gYVX0QkplIU1FWAii8iEj+pCWoVX0QkrlIT1Cq+iEhcpSaoYaj4cqz7ZNijiIjkLWVBnSu+bGnRUbWIxEeqgvqtKr6ISAylKqgry4o575wZKr6ISKykKqgBGhbNVvFFRGIlfUGt4ouIxEwKg7oKUPFFROIjdUGt4ouIxE3qglrFFxGJm9QFNaj4IiLxktKgVvFFROIj76A2syIz22JmGwo50FRQ8UVE4mQiR9S3AzsLNchUUvFFROIkr6A2s3rg/cC9hR1n6qj4IiJxke8R9d3A7wHZU+1gZmvNrMnMmtrb2ydjtoJS8UVE4mLcoDaz64GD7r75dPu5+zp3b3T3xrq6ukkbsFBUfBGRuMjniPpK4AYz2wM8CFxtZt8o6FRTQMUXEYmLcYPa3T/j7vXuvhi4Bfihu3+k4JMV2EDxZUvL0bBHERE5rVSeRz2gYWEVLx/sVPFFRCJtQkHt7j929+sLNcxUU/FFROIg1UfUKr6ISBykOqhVfBGROEh1UIOKLyISfQpqFV9EJOIU1Cq+iEjEpT6oVXwRkahLfVCr+CIiUZf6oAYVX0Qk2hTUqPgiItGmoEbFFxGJNgU1Kr6ISLQpqAMqvohIVCmoAyq+iEhUKagDKr6ISFQpqAMDxRetU4tI1CioAwPFF535ISJRo6AeRsUXEYkiBfUwq1R8EZEIUlAPs1LFFxGJIAX1MCq+iEgUKahHUfFFRKJGQT2Kii8iEjUK6lHetqSaoozxG9/YzFO7OsIeR0REQT3agupy/uXXLqUv69yy7il+f/3zOl1PREKloB7DlW+u5fE7VvOJdyxlfXMr13zxJzz63H7ctW4tIlNPQX0K00uLuOu68/mP37qSeVXTuO2BLdz6z020HT0R9mgikjIK6nFcMG8Wj/zmlfzR9St4alcH7/7iT7hv0276dVaIiEwRBXUeijLGrVct4fufXs2lS6r58w07uOmrP2PngeNhjyYiKaCgnoD62eXc//Gf4+8/tIq2I9184B828fmNL/L6yf6wRxORBFNQT5CZccPKefznne/gpob5fPXHr/Deu5/gyZcPhT2aiCSUgvoMVZWX8tdrVvKvv/42DPjwvU/zO99+jiNdvWGPJiIJo6A+S1csrWXjHav55M8v5d+3tPGuL/6E72xt06l8IjJpxg1qM5tmZs+Y2XNmtt3M/mwqBouTaSVF/O57z+fR266ivrqc2x/cysfvf5aWw91hjyYiCZDPEXUPcLW7rwQuBq41s8sKOlVMvWXuTB7+jSv40w+soGnPYd7zd09w70930defDXs0EYmxcYPacwY+oagkuOjf9adQlDE+fuUSfnDnO7hiaQ1/8d2dfPArP2Nb27GwRxORmMprjdrMisxsK3AQ+IG7Pz3GPmvNrMnMmtrb2yd5zPiZVzWde3+lkXt+uYEDx17nxnue5HOP7eREr07lE5GJsYm86WVmVcAjwG3uvu1U+zU2NnpTU9PZT5cQx7pP8lcbd/LAMy0sqJ7OX37wIt6+rC7ssUQkQsxss7s3jrVtQmd9uPtR4EfAtZMwV2rMKi/hcze9lQfXXkZJJsNHv/4Md/7bVg7rVD4RyUM+Z33UBUfSmNl04N3AiwWeK5EuO7eGx25/O5+6+s08+vx+rvnbH/Nwc6tO5ROR08rniHou8CMzex54ltwa9YbCjpVc00qKuPM9y/nup97OktoK7vzWc3zsvmfY16FT+URkbBNao86X1qjzk80633x6L5/f+BJ92Syfftd53HrVEoqL1EMSSZtJW6OWyZXJGB+9fDE/uHM1b19Wx+e+9yI3fPlJXmjVqXwiMkRBHQFzZ03nHz/WyNc+0sChzh5uvGcTf7FhB929fWGPJiIRUBz2ADLk2gvncsWba/n8917k3k27eeyFA3z4skXc1DCfubOmhz2eiIREa9QR9eyew/zN4y/xzO7DmMFVb65lzSX1vGfFHKaXFoU9nohMstOtUSuoI25fRzcPNbfyUHMrrUdOMKOsmOtXzuXmhnouWTQbMwt7RBGZBArqBMhmnad3H+ah5lYee+EA3b39LKmt4OaG+XywoZ75VVoaEYkzBXXCdPX08b1tr7J+cwtP7cotjVyxtIY1l9Tz3gvmUF6qtx5E4kZBnWAth7t5uLmN9c0ttBw+QWVZMe+7aA5rLlnAzy3W0ohIXCioUyCbdZ7dk1sa+e7zB+jq7WdRTTk3N9RzU8N86meXhz2iiJyGgjplunv72LjtVdZvbuVnr3QAcPm5uaWR6y7S0ohIFCmoU6z1SDePNLexvrmVvR3dVJQWcd1Fc1lzST2XLq4mk9HSiEgUKKgFd2fz3iOs39zKhucP0NnTx4Lq6dzcUM/NDfUsqNbSiEiYFNQywonefh7fnlsaefKVQ7jD25ZUs+aSet530VwqyrQ0IjLVFNRySvuPnuCRLW2s39zK7kNdTC8p4rqL5rDmknouW1KjpRGRKaKglnG5O837jrB+cxsbntvPaz19zK+azs0N87nh4nmcW1up0BYpIAW1TMjrJ4eWRja9nFsamVFWzIp5M7lw/iwunD+TC+fN4ty6SooU3iKT4nRBrcVIeYNpJUXcePF8brx4PgeOneAnL7Wzff9xtu0/xjee2ktPXxaA6SVFvGXujFx4z5vFBfNnsuxNMygt1qfnikwmHVHLhPT1Z9l1qIsXWo+xbf8xtrcdZ/v+Y3T19gNQWpRh+ZwZuaPuIMCXz5nBtBJ94p/I6WjpQwoqm3X2dHSxbf9xtrflAnxb23GOnTgJQFHGWPamyiC4cwH+lrkzdXaJyDAKaply7k7rkRNsD0I7F97HONTZC4AZnFtbMWLZ5IJ5s5g1vSTkyUXCoTVqmXJmxoLqchZUl3PthXOBXHgffK2HbW1D4f3s7sN8Z+v+wcctrC7nwiC0B47AayrLwvrPEIkEBbVMGTPjnJnTOGfmNK55yzmD93d09gy+Wbk9CPDHXnh1cPvcWdO4YN4slr6pgtqKMmoqS6muKKW2cuh6WbHWwCW5FNQSuprKMlafV8fq8+oG7zt24iQ79h8Plk6O8ULbMZ74n3Z6+7NjPseMsuLB0K6pLKOmopSaylJqgmAf+lnK7IpSSop0ZorEh4JaImnW9BIuX1rD5UtrBu9zdzp7+ujo7KWjq5eOzh46uno53NXLoc4eOjpz11sOd7O15SiHu3rpz479Hsys6SXUVJZSW1EWhPvIgB88Yq8opaq8VOeLS6gU1BIbZsaMaSXMmFbC4tqKcffPZp3jr5/kUBDgHZ09HOrq5XBnLx1dPUHg9/BKeyfP7OnlSHcvY723bgbV5aWDgV5RWkxJUYaS4gwlGaOkKENxUe5nyeDPoevFRRlKi4ziUfcPPK60KENxxoLny1BSHGwPrhdnMrl9hv0Z+kKIdFFQS2JlMkZVee6IOB/9WedId+9ggHd05sL9cFcvh7qGrr96/HVO9mfp63d6+7Mjrvf1e+72KY7kJ0txxoYFdy68izOn+uUQ3JcZtf+IbcEviExm2C+eUc+VGfaLZdhjM2ZkLHcaZiZjFJnl7svk7iuy3C+WoevB/Rl7w2MzFjw+w7Dn0S8lBbVIoChj1FaWUVtZBsw4q+dyd072O33ZLCf7ghAPrp/Mjh3uw6/nLk7fsOsnR13vyzq9fbnnHf1cg392cL2zp2/Ec/dlnZN9WU5mfXCWgW0F/h1zRt4Q8qMC38wYiPOBf2wM3DN0e2D7yOAf3J7n4wYfPcb26vJSvvWJy8/mP3VMCmqRAjAzSouNUjKQ3wF9ZPRnh34R9L3hF8iwYM9mOdmXpd8d99zjcted/mzudtZzl8HrWeh3Jxvsm/XcEtXIfQn29dPsy+DzDvyZwODS1eBPRt3P2Nt5w3Y/xf5jbx+4MmNaYSJVQS0iI+SOUnW6Y5ToHCURkYhTUIuIRJyCWkQk4sYNajNbYGY/MrMdZrbdzG6fisFERCQnnzcT+4DfdvdmM5sBbDazH7j7jgLPJiIi5HFE7e4H3L05uP4asBOYX+jBREQkZ0Jr1Ga2GFgFPD3GtrVm1mRmTe3t7ZM0noiI5B3UZlYJPATc4e7HR29393Xu3ujujXV1dW98AhEROSN5fcOLmZUAG4DH3f2LeezfDuw9w5lqgUNn+Nik0Wsxkl6PkfR6DEnCa7HI3cc8yh03qC1XcP9n4LC73zH5s73hz2s61dfRpI1ei5H0eoyk12NI0l+LfJY+rgQ+ClxtZluDy/sKPJeIiATGPT3P3Tcx7AOjRERkakWxmbgu7AEiRK/FSHo9RtLrMSTRr0VebyaKiEh4onhELSIiwyioRUQiLjJBbWbXmtlLZvaymd0V9jxh0gdhvZGZFZnZFjPbEPYsYTOzKjNbb2YvmtlOM5v8736KETP7dPD3ZJuZPWBm08KeabJFIqjNrAi4B7gOWAF8yMxWhDtVqAY+CGsFcBnwyZS/HgC3k/ucGYEvARvd/XxgJSl+XcxsPvApoNHdLwSKgFvCnWryRSKogUuBl919l7v3Ag8CN4Y8U2j0QVgjmVk98H7g3rBnCZuZzQJWA18HcPdedz8a6lDhKwamm1kxUA7sD3meSReVoJ4PtAy73UqKg2m4030QVorcDfwekA15jihYArQD9wdLQfeaWUXYQ4XF3duALwD7gAPAMXf/frhTTb6oBLWMYbwPwkoDM7seOOjum8OeJSKKgQbgq+6+CugCUvuejpnNJvev7yXAPKDCzD4S7lSTLypB3QYsGHa7PrgvtYIPwnoI+Ka7Pxz2PCG6ErjBzPaQWxK72sy+Ee5IoWoFWt194F9Y68kFd1q9C9jt7u3ufhJ4GLgi5JkmXVSC+llgmZktMbNScm8G/EfIM4Um+CCsrwM78/m0wiRz98+4e727Lyb3/8UP3T1xR0z5cvdXgRYzWx7cdQ2Q5m9b2gdcZmblwd+ba0jgm6v5fBVXwbl7n5n9FvA4uXdt73P37SGPFaaBD8J6wcy2Bvf9gbs/Ft5IEiG3Ad8MDmp2Ab8a8jyhcfenzWw90EzubKktJLBOrgq5iEjERWXpQ0RETkFBLSIScQpqEZGIU1CLiEScglpEJOIU1CIiEaegFhGJuP8H/JItxuWxytUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_img = torch.nn.CrossEntropyLoss()\n",
    "loss_txt = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(sparse_mlm.parameters(), lr=1e-4,betas=(0.9,0.98),eps=1e-6,weight_decay=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[2,5,8], gamma=0.1)\n",
    "\n",
    "epochs = 10\n",
    "totalbatches = int(len(dataset) / batchsize)\n",
    "logit_scale = 100 #model.logit_scale.exp().item()\n",
    "alpha = 1\n",
    "lambda1 = 0.0001\n",
    "cliplosses = []\n",
    "sparselosses = []\n",
    "test_loss=[[rec1],[rec5],[rec10]]\n",
    "for epoch in range(0, epochs):\n",
    "    i = 0\n",
    "    cliploss_total = 0\n",
    "    sparseloss_total = 0\n",
    "    for batch in train_dataloader:\n",
    "        if i % 25 == 0:\n",
    "            print(\"epoch:\", epoch, \"batch:\", i, \"/\", totalbatches, end='\\r')\n",
    "       \n",
    "        optimizer.zero_grad()\n",
    "        ims, caps = batch\n",
    "        \n",
    "        # sparse encoding\n",
    "        sparse_ims = sparse_mlm(ims)\n",
    "        sparse_caps = sparse_mlm(caps)\n",
    "        \n",
    "        l1_regularization = torch.norm(sparse_ims, 1) + torch.norm(sparse_caps, 1)\n",
    "\n",
    "        # determine logits\n",
    "        sparse_ims = sparse_ims / sparse_ims.norm(dim=-1, keepdim=True)\n",
    "        sparse_caps = sparse_caps / sparse_caps.norm(dim=-1, keepdim=True)\n",
    "        logits_per_image = logit_scale * sparse_ims @ sparse_caps.t()\n",
    "        logits_per_text = logits_per_image.t()\n",
    "        \n",
    "        # compute losses\n",
    "        ground_truth = torch.arange(len(ims),dtype=torch.long,device=device)\n",
    "        \n",
    "        clip_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "        total_loss = alpha * clip_loss + lambda1 * l1_regularization\n",
    "        \n",
    "        cliploss_total += clip_loss.item()\n",
    "        sparseloss_total += l1_regularization.item()\n",
    "    \n",
    "        total_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        i+=1\n",
    "        \n",
    "    scheduler.step()    \n",
    "    cliplosses.append(cliploss_total)\n",
    "    sparselosses.append(sparseloss_total)\n",
    "    print(\"loss:\", cliploss_total)\n",
    "    \n",
    "    rec1,rec5,rec10 = test_sparse_performance(sparse_mlm, test_images, test_features)\n",
    "    test_loss[0].append(rec1)\n",
    "    test_loss[1].append(rec5)\n",
    "    test_loss[2].append(rec10)\n",
    "    print(\"R@1:\", rec1)\n",
    "    print(\"\")\n",
    "\"\"\"\n",
    "    torch.save({\n",
    "        'epoch':epoch,\n",
    "        'model_state_dict': sparse_mlm.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': total_loss,\n",
    "        }, f\"Models/\" + str(epoch) + \".pt\")\n",
    "\"\"\"      \n",
    "print(\"\")\n",
    "print(\"done\")\n",
    "\n",
    "plt.plot(cliplosses)\n",
    "plt.title('CLIP loss')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(sparselosses)\n",
    "plt.title('L1-reg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor(0.5302),\n",
       "  tensor(0.5934),\n",
       "  tensor(0.5816),\n",
       "  tensor(0.5964),\n",
       "  tensor(0.5912),\n",
       "  tensor(0.5908),\n",
       "  tensor(0.5930),\n",
       "  tensor(0.5926),\n",
       "  tensor(0.5926),\n",
       "  tensor(0.5922),\n",
       "  tensor(0.5920)],\n",
       " [tensor(0.7908),\n",
       "  tensor(0.8498),\n",
       "  tensor(0.8554),\n",
       "  tensor(0.8638),\n",
       "  tensor(0.8622),\n",
       "  tensor(0.8592),\n",
       "  tensor(0.8584),\n",
       "  tensor(0.8598),\n",
       "  tensor(0.8602),\n",
       "  tensor(0.8598),\n",
       "  tensor(0.8592)],\n",
       " [tensor(0.8626),\n",
       "  tensor(0.9102),\n",
       "  tensor(0.9176),\n",
       "  tensor(0.9204),\n",
       "  tensor(0.9210),\n",
       "  tensor(0.9204),\n",
       "  tensor(0.9200),\n",
       "  tensor(0.9208),\n",
       "  tensor(0.9210),\n",
       "  tensor(0.9206),\n",
       "  tensor(0.9206)]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9973, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    total_nonzero = 0\n",
    "    i = 0\n",
    "    for image in test_images:\n",
    "        sparse = sparse_mlm(image)\n",
    "        total_nonzero += (len(sparse) - torch.count_nonzero(sparse))/len(sparse)\n",
    "        i += 1\n",
    "        \n",
    "    for caption in test_features:\n",
    "        sparse = sparse_mlm(caption)\n",
    "        total_nonzero += (len(sparse) - torch.count_nonzero(sparse))/len(sparse)\n",
    "        i += 1\n",
    "        \n",
    "    print(total_nonzero/i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(test_loss[0])\n",
    "plt.title('recall@1 testset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open the json file for annotations\n",
    "f = open(\"C:/Users/Lalashops/Desktop/MasterThesis/random images/vocab.json\", encoding = 'utf-8')\n",
    "vocab = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_cap = sparse_mlm(test_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reflected</w> tensor(0.5257, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "amphibious</w> tensor(0.4498, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "floating</w> tensor(0.2255, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "minded</w> tensor(0.2152, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "macedonia</w> tensor(0.1916, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "peterson</w> tensor(0.1734, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "shaq</w> tensor(0.1689, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "manipu tensor(0.1620, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "that tensor(0.1543, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "geon</w> tensor(0.1481, device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for idx in torch.topk(sparse_cap, 10).indices:\n",
    "    print(list(vocab.keys())[list(vocab.values()).index(idx)], sparse_cap[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
