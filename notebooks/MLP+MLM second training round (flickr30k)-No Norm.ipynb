{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import glob\n",
    "import time\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\lib\\site-packages\\transformers\\models\\clip\\processing_clip.py:142: FutureWarning: `feature_extractor` is deprecated and will be removed in v5. Use `image_processor` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "preprocess = processor.feature_extractor\n",
    "tokenizer = processor.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "preprocess = processor.feature_extractor\n",
    "tokenizer = processor.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"Models/model_clip-vit-large-patch14/model.pt\", map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "c:\\python\\lib\\site-packages\\transformers\\models\\clip\\processing_clip.py:142: FutureWarning: `feature_extractor` is deprecated and will be removed in v5. Use `image_processor` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14-336\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14-336\")\n",
    "\n",
    "preprocess = processor.feature_extractor\n",
    "tokenizer = processor.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLM(nn.Module):\n",
    "    def __init__(self, dense_size, vocab_size):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(dense_size, vocab_size)\n",
    "        \n",
    "    def forward(self, dense_vec):\n",
    "        term_importances = torch.log1p(torch.relu(self.linear(dense_vec)))\n",
    "        return term_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "4500 , time since last:  0.50863718986511234\n",
      "tensor(0.5214)\n",
      "tensor(0.8196)\n",
      "tensor(0.9000)\n"
     ]
    }
   ],
   "source": [
    "#before training\n",
    "rec1,rec5,rec10 = test_sparse_performance(text_encoder, image_encoder, test_images, test_features)\n",
    "\n",
    "print(rec1)\n",
    "print(rec5)\n",
    "print(rec10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sparse_performance(text_model, image_model, test_ims, test_cap):\n",
    "    with torch.no_grad():\n",
    "        x, _ = test_ims.shape\n",
    "        encoded_ims = torch.empty(int(x), 49408).to(device)\n",
    "        for i in range(0, len(test_ims), 64):\n",
    "            encoded_ims[i:i+64] = image_model(test_ims[i:i+64])\n",
    "\n",
    "\n",
    "        encoded_caps = torch.empty(int(x*5), 49408)\n",
    "        for i in range(0, len(encoded_caps), 64):\n",
    "            encoded_caps[i:i+64] = text_model(test_cap[i:i+64])\n",
    "            \n",
    "        \n",
    "        encoded_ims = encoded_ims / encoded_ims.norm(dim=-1, keepdim=True)\n",
    "        encoded_caps = encoded_caps / encoded_caps.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        recall_1, recall_5, recall_10 = [],[],[]\n",
    "        i = 0\n",
    "        j = 0\n",
    "        image_id = 0\n",
    "        print(len(encoded_caps))\n",
    "        t = time.time()\n",
    "        for text_feature in encoded_caps:\n",
    "            if (j%500) == 0:\n",
    "                print(j, \", time since last: \", time.time() - t, end='\\r')\n",
    "                t = time.time()\n",
    "            similarity = (100.0 * text_feature.to(device) @ encoded_ims.T).softmax(dim=-1)\n",
    "            _, indices = similarity.topk(10)\n",
    "            \n",
    "            recall_1.append(image_id in indices[0])\n",
    "            recall_5.append(image_id in indices[:5])\n",
    "            recall_10.append(image_id in indices)\n",
    "\n",
    "            i += 1\n",
    "            j += 1\n",
    "            if i == 5:\n",
    "                i = 0\n",
    "                image_id += 1\n",
    "\n",
    "        print(\"\")\n",
    "        return torch.Tensor(recall_1).mean(), torch.Tensor(recall_5).mean(), torch.Tensor(recall_10).mean()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainBatches():\n",
    "    def __init__(self, image_vectors, captions):\n",
    "\n",
    "        self.images = image_vectors\n",
    "        self.captions  = captions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        caption = self.captions[idx]\n",
    "        return image,caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open the json file for annotations\n",
    "f = open('datasets/flickr30k/dataset_flickr30k.json')\n",
    "data = json.load(f)\n",
    "\n",
    "# load all captions\n",
    "caps = [x['sentences'] for x in data['images'] if x.get('split') == 'train']\n",
    "test_caps = [x['sentences'] for x in data['images'] if x.get('split') == 'test']\n",
    "\n",
    "files = []\n",
    "test_files = []\n",
    "for x in data['images']:\n",
    "    if x.get('split') == 'train':\n",
    "        files.append('datasets/flickr30k/train/' + x['filename'])\n",
    "    \n",
    "    elif x.get('split') == 'test':\n",
    "        test_files.append('datasets/flickr30k/test/' + x['filename'])\n",
    "            \n",
    "captions = []\n",
    "for x in caps:\n",
    "    for y in x:\n",
    "        captions.append(y)\n",
    "        \n",
    "test_captions = []\n",
    "for x in test_caps:\n",
    "    for y in x:\n",
    "        test_captions.append(y)\n",
    "    \n",
    "captions = [x.get('raw') for x in captions]\n",
    "test_captions = [x.get('raw') for x in test_captions]\n",
    "\n",
    "# these captions are too long and need adjusting\n",
    "captions[13035] = 'Four young adults sit outside on a wooden deck near a building around a small round table, while another person stands on the edge of the deck, leaning on the wooden railing, with the sun shining on one of them, one holding a cellphone out in front of himself and another holding a green and red soda can.'\n",
    "captions[14580] = 'A man wearing a helmet, red pants with white and a white and red shirt is on a small bicycle using only his hands, while another man wearing a light blue shirt with dark blue trim and black pants with red stripes is standing nearby, gesturing toward the first man and holding a small figurine.'\n",
    "captions[120165] = 'In this photo there is a man in a dirty white shirt and a black hat with yellow stained teeth, he looks happy and it appears that he is also repairing something.'\n",
    "test_captions[3905] = 'Two boys are looking upwards with their arms streteched to the sky, the boy on the left is wearing a blue vest jacket with a gray shirt, black jogging pants and a hat, and the boy on the right is wearing a silver vest jacket, with blue long-sleeved undershirt, gray pants, black tennis shoes and has black short hair and glasses.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = files[0:10]\n",
    "captions = captions[0:50]\n",
    "\n",
    "test_files = test_files[0:10]\n",
    "test_captions = test_captions[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28900 / 29000 / 29000 / 29000 / 29000 / 29000 / 29000 / 29000 / 29000 / 29000 / 29000 / 29000 / 29000 / 29000\n",
      "done\n",
      "293.0039150714874\n"
     ]
    }
   ],
   "source": [
    "# encode images\n",
    "L = len(files)\n",
    "batch_size = 64\n",
    "t1 = time.time()\n",
    "with torch.no_grad():\n",
    "    encoded_ims = torch.Tensor()\n",
    "    for i in range(0, L, batch_size):\n",
    "        print(i,\"/\",L, end='\\r')\n",
    "        images = torch.Tensor().to(device)\n",
    "        for x in range(batch_size):\n",
    "            if (i + x) < L:\n",
    "                image = preprocess(Image.open(files[i+x]), return_tensors='pt')['pixel_values'].to(device)\n",
    "                images = torch.cat((images, image), 0)\n",
    "                \n",
    "        ims = model.vision_model(images).pooler_output\n",
    "        ims = model.visual_projection(ims)\n",
    "        encoded_ims = torch.cat((encoded_ims, ims.to(\"cpu\")), 0)\n",
    "\n",
    "p = 0\n",
    "encoded_images = torch.Tensor().to(device)\n",
    "for image in encoded_ims:\n",
    "    if (p%100) == 0:\n",
    "        print(p,\"/\",L, end='\\r')\n",
    "        \n",
    "    encoded_images = torch.cat((encoded_images, image.to(device).repeat(5,1)), 0)\n",
    "    p += 1\n",
    "    \n",
    "print(\"\")\n",
    "print(\"done\")\n",
    "print(time.time() - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vitb32, vitL14@336, vitL14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_images = torch.load(\"pt datafiles/flickr/MLP/encoded_images_vitb32.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([145000, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoded_images, \"encoded_images_vitb32.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode captions\n",
    "L = len(captions)\n",
    "batch_size = 128\n",
    "t1 = time.time()\n",
    "with torch.no_grad():\n",
    "    text_features = torch.Tensor().to(device)\n",
    "    for i in range(0, L, batch_size):\n",
    "        print(i,\"/\",L, end='\\r')\n",
    "        text = tokenizer(captions[i:i+batch_size], padding=True, return_tensors='pt').to(device)\n",
    "        \n",
    "        text_output = model.text_model(**text).pooler_output\n",
    "        text_output = model.text_projection(text_output)      \n",
    "        text_features = torch.cat((text_features, text_output), 0)\n",
    "        \n",
    "print(\"\")\n",
    "print(\"done\")\n",
    "print(time.time() - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features = torch.load(\"pt datafiles/flickr/MLP/encoded_captions_vitb32.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1201075200\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(text_features, \"encoded_captions_vitL14@336.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "992 / 1000\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# encode images\n",
    "L = len(test_files)\n",
    "batch_size = 128\n",
    "t1 = time.time()\n",
    "with torch.no_grad():\n",
    "    test_ims = torch.Tensor()\n",
    "    for i in range(0, L, batch_size):\n",
    "        print(i,\"/\",L, torch.cuda.memory_allocated(), end='\\r')\n",
    "        images = torch.Tensor().to(device)\n",
    "        for x in range(batch_size):\n",
    "            if (i + x) < L:\n",
    "                image = preprocess(Image.open(test_files[i+x]), return_tensors='pt')['pixel_values'].to(device)\n",
    "                images = torch.cat((images, image), 0)\n",
    "                \n",
    "        ims = model.vision_model(images).pooler_output\n",
    "        ims = model.visual_projection(ims)\n",
    "        test_ims = torch.cat((test_ims, ims.to(\"cpu\")), 0)\n",
    "\n",
    "test_images = test_ims.to(device)\n",
    "print(\"\")\n",
    "print(\"done\")\n",
    "print(time.time() - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = torch.load(\"pt datafiles/flickr/MLP/test_images_vitb32.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1203172352\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(test_images, \"test_images_vitL14@336.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4992 / 5000/ 5000 / 5000 / 5000\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# encode test captions\n",
    "L = len(test_captions)\n",
    "batch_size = 128\n",
    "with torch.no_grad():\n",
    "    test_features = torch.Tensor().to(device)\n",
    "    for i in range(0, L, batch_size):\n",
    "        print(i,\"/\",L, end='\\r')\n",
    "        text = tokenizer(test_captions[i:i+batch_size], padding=True, return_tensors='pt').to(device)\n",
    "        \n",
    "        text_output = model.text_model(**text).pooler_output\n",
    "        text_output = model.text_projection(text_output)      \n",
    "        test_features = torch.cat((test_features, text_output), 0)\n",
    "        \n",
    "print(\"\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = torch.load(\"pt datafiles/flickr/MLP/test_captions_vitb32.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(test_features, \"test_captions_vitL14@336.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 64\n",
    "\n",
    "dataset = TrainBatches(encoded_images, text_features)\n",
    "train_dataloader = DataLoader(dataset, batch_size = batchsize, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_text_size = model.text_projection.weight.shape[0]\n",
    "dense_image_size = model.visual_projection.weight.shape[0]\n",
    "vocab_size = model.text_model.config.vocab_size\n",
    "\n",
    "text_encoder = MLM(dense_text_size, vocab_size).to(device)\n",
    "image_encoder = MLM(dense_image_size, vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_checkpoint = torch.load(\"Models/8.pt\", map_location='cpu')\n",
    "\n",
    "image_encoder = MLM(dense_image_size, vocab_size).to(device)\n",
    "image_encoder.load_state_dict(im_checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "4500 , time since last:  0.47563505172729496\n",
      "tensor(0.5214)\n",
      "tensor(0.8196)\n",
      "tensor(0.9000)\n"
     ]
    }
   ],
   "source": [
    "#before training\n",
    "rec1,rec5,rec10 = test_sparse_performance(text_encoder, image_encoder, test_images, test_features)\n",
    "\n",
    "print(rec1)\n",
    "print(rec5)\n",
    "print(rec10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 batch: 2250 / 2265\n",
      "loss: 1988.5013686418533\n",
      "testing recall scores..\n",
      "5000\n",
      "4500 , time since last:  0.48062872886657715\n",
      "R@1: tensor(0.4114)\n",
      "epoch done\n",
      "\n",
      "epoch: 1 batch: 2250 / 2265\n",
      "loss: 976.5060137659311\n",
      "testing recall scores..\n",
      "5000\n",
      "4500 , time since last:  0.48062896728515625\n",
      "R@1: tensor(0.4604)\n",
      "epoch done\n",
      "\n",
      "epoch: 2 batch: 2250 / 2265\n",
      "loss: 878.0527661889791\n",
      "testing recall scores..\n",
      "5000\n",
      "4500 , time since last:  0.51563835144042975\n",
      "R@1: tensor(0.4744)\n",
      "epoch done\n",
      "\n",
      "epoch: 3 batch: 2250 / 2265\n",
      "loss: 843.093530267477\n",
      "testing recall scores..\n",
      "5000\n",
      "4500 , time since last:  0.5016362667083748\n",
      "R@1: tensor(0.4750)\n",
      "epoch done\n",
      "\n",
      "epoch: 4 batch: 2250 / 2265\n",
      "loss: 823.2822391688824\n",
      "testing recall scores..\n",
      "5000\n",
      "4500 , time since last:  0.48563003540039066\n",
      "R@1: tensor(0.4858)\n",
      "epoch done\n",
      "\n",
      "epoch: 5 batch: 2250 / 2265\n",
      "loss: 788.8231510519981\n",
      "testing recall scores..\n",
      "5000\n",
      "4500 , time since last:  0.47916412353515625\n",
      "R@1: tensor(0.4862)\n",
      "epoch done\n",
      "\n",
      "epoch: 6 batch: 2250 / 2265\n",
      "loss: 781.4137491136789\n",
      "testing recall scores..\n",
      "5000\n",
      "4500 , time since last:  0.49063301086425785\n",
      "R@1: tensor(0.4862)\n",
      "epoch done\n",
      "\n",
      "epoch: 7 batch: 2250 / 2265\n",
      "loss: 779.9515285864472\n",
      "testing recall scores..\n",
      "5000\n",
      "4500 , time since last:  0.48049163818359375\n",
      "R@1: tensor(0.4892)\n",
      "epoch done\n",
      "\n",
      "epoch: 8 batch: 2250 / 2265\n",
      "loss: 780.0592139363289\n",
      "testing recall scores..\n",
      "5000\n",
      "4500 , time since last:  0.48046183586120605\n",
      "R@1: tensor(0.4878)\n",
      "epoch done\n",
      "\n",
      "epoch: 9 batch: 2250 / 2265\n",
      "loss: 777.272539511323\n",
      "testing recall scores..\n",
      "5000\n",
      "4500 , time since last:  0.48140335083007816\n",
      "R@1: tensor(0.4888)\n",
      "epoch done\n",
      "\n",
      "epoch: 10 batch: 2250 / 2265\n",
      "loss: 772.1966587156057\n",
      "testing recall scores..\n",
      "5000\n",
      "4500 , time since last:  0.48763203620910645\n",
      "R@1: tensor(0.4894)\n",
      "epoch done\n",
      "\n",
      "epoch: 11 batch: 2250 / 2265\n",
      "loss: 771.4698823690414\n",
      "testing recall scores..\n",
      "5000\n",
      "4500 , time since last:  0.48263096809387207\n",
      "R@1: tensor(0.4890)\n",
      "epoch done\n",
      "\n",
      "epoch: 12 batch: 2250 / 2265\n",
      "loss: 771.6730451583862\n",
      "testing recall scores..\n",
      "5000\n",
      "4500 , time since last:  0.48263001441955566\n",
      "R@1: tensor(0.4890)\n",
      "epoch done\n",
      "\n",
      "epoch: 13 batch: 2250 / 2265\n",
      "loss: 772.7361086159945\n",
      "testing recall scores..\n",
      "5000\n",
      "4500 , time since last:  0.47962951660156254\n",
      "R@1: tensor(0.4898)\n",
      "epoch done\n",
      "\n",
      "epoch: 14 batch: 2250 / 2265\n",
      "loss: 772.8386906832457\n",
      "testing recall scores..\n",
      "5000\n",
      "4500 , time since last:  0.48412775993347175\n",
      "R@1: tensor(0.4894)\n",
      "epoch done\n",
      "\n",
      "\n",
      "done\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_img = torch.nn.CrossEntropyLoss()\n",
    "loss_txt = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(text_encoder.parameters(), lr=1e-4,betas=(0.9,0.98),eps=1e-6,weight_decay=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[5,10], gamma=0.1)\n",
    "\n",
    "vocab_size = model.text_model.config.vocab_size\n",
    "epochs = 15\n",
    "totalbatches = int(len(dataset) / batchsize)\n",
    "losses=[]\n",
    "test_loss=[[rec1],[rec5],[rec10]]\n",
    "for epoch in range(0, epochs):\n",
    "    i = 0\n",
    "    batch_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        if i % 25 == 0:\n",
    "            print(\"epoch:\", epoch, \"batch:\", i, \"/\", totalbatches, end='\\r')\n",
    "       \n",
    "        optimizer.zero_grad()\n",
    "        dense_images, dense_caps = batch\n",
    "        \n",
    "        # sparse encoding\n",
    "        with torch.no_grad():\n",
    "            sparse_ims = image_encoder(dense_images)\n",
    "            \n",
    "        sparse_caps = text_encoder(dense_caps)\n",
    "\n",
    "        # determine logits\n",
    "        logits_per_image = sparse_ims @ sparse_caps.t()\n",
    "        logits_per_text = logits_per_image.t()\n",
    "        \n",
    "        # compute losses\n",
    "        ground_truth = torch.arange(len(dense_images),dtype=torch.long,device=device)\n",
    "        \n",
    "        loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "            \n",
    "        batch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        i+=1\n",
    "        \n",
    "    scheduler.step()\n",
    "    losses.append(batch_loss)\n",
    "    print(\"\")\n",
    "    print(\"loss:\", batch_loss)\n",
    "    \n",
    "    print(\"testing recall scores..\")\n",
    "    recall1,recall5,recall10 = test_sparse_performance(text_encoder, image_encoder, test_images, test_features)\n",
    "    test_loss[0].append(recall1)\n",
    "    test_loss[1].append(recall5)\n",
    "    test_loss[2].append(recall10)\n",
    "    print(\"R@1:\", recall1)\n",
    "    print(\"epoch done\")\n",
    "    print(\"\")\n",
    "\"\"\"\n",
    "    torch.save({\n",
    "        'epoch':epoch,\n",
    "        'model_state_dict': sparse_mlm.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': total_loss,\n",
    "        }, f\"Models/\" + str(epoch) + \".pt\")\n",
    "\"\"\"      \n",
    "print(\"\")\n",
    "print(\"done\")\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.title('CLIP loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor(0.0006),\n",
       "  tensor(0.4114),\n",
       "  tensor(0.4604),\n",
       "  tensor(0.4744),\n",
       "  tensor(0.4750),\n",
       "  tensor(0.4858),\n",
       "  tensor(0.4862),\n",
       "  tensor(0.4862),\n",
       "  tensor(0.4892),\n",
       "  tensor(0.4878),\n",
       "  tensor(0.4888),\n",
       "  tensor(0.4894),\n",
       "  tensor(0.4890),\n",
       "  tensor(0.4890),\n",
       "  tensor(0.4898),\n",
       "  tensor(0.4894)],\n",
       " [tensor(0.0048),\n",
       "  tensor(0.6038),\n",
       "  tensor(0.6454),\n",
       "  tensor(0.6594),\n",
       "  tensor(0.6572),\n",
       "  tensor(0.6690),\n",
       "  tensor(0.6670),\n",
       "  tensor(0.6668),\n",
       "  tensor(0.6702),\n",
       "  tensor(0.6710),\n",
       "  tensor(0.6712),\n",
       "  tensor(0.6704),\n",
       "  tensor(0.6704),\n",
       "  tensor(0.6706),\n",
       "  tensor(0.6704),\n",
       "  tensor(0.6700)],\n",
       " [tensor(0.0098),\n",
       "  tensor(0.6140),\n",
       "  tensor(0.6540),\n",
       "  tensor(0.6676),\n",
       "  tensor(0.6660),\n",
       "  tensor(0.6776),\n",
       "  tensor(0.6752),\n",
       "  tensor(0.6752),\n",
       "  tensor(0.6790),\n",
       "  tensor(0.6802),\n",
       "  tensor(0.6790),\n",
       "  tensor(0.6780),\n",
       "  tensor(0.6778),\n",
       "  tensor(0.6782),\n",
       "  tensor(0.6780),\n",
       "  tensor(0.6776)]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "tensor(0.3365, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    total_nonzero = 0\n",
    "    i = 0\n",
    "    for feature in text_features:\n",
    "        sparse = text_encoder(feature)\n",
    "        total_nonzero += (len(sparse) - torch.count_nonzero(sparse))/len(sparse)\n",
    "        i += 1\n",
    "        if (i%10000) == 0:\n",
    "            print(i)\n",
    "        \n",
    "    print(total_nonzero/i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(test_loss[0][1:])\n",
    "plt.title('CLIP loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open the json file for annotations\n",
    "f = open(\"C:/Users/Lalashops/Desktop/MasterThesis/random images/vocab.json\", encoding = 'utf-8')\n",
    "vocab = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_encoding_cap = text_encoder(text_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Two young guys with shaggy hair look at their hands while hanging out in the yard.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_encoding_im = image_encoder(encoded_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_encoding = image_encoder(test_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "l = torch.topk(sparse_encoding_im, 100).indices\n",
    "for idx in torch.topk(sparse_encoding_cap, 20).indices:\n",
    "    print(idx in l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([0.8103, 0.7990, 0.7422, 0.7382, 0.7256, 0.7129, 0.7085, 0.6937, 0.6635,\n",
       "        0.6454, 0.5775, 0.5746, 0.5698, 0.5514, 0.5342, 0.5319, 0.5231, 0.5006,\n",
       "        0.4979, 0.4936, 0.4862, 0.4862, 0.4760, 0.4660, 0.4566, 0.4560, 0.4543,\n",
       "        0.4487, 0.4420, 0.4370, 0.4258, 0.4125, 0.4083, 0.4047, 0.4037, 0.3980,\n",
       "        0.3976, 0.3960, 0.3948, 0.3827, 0.3790, 0.3789, 0.3770, 0.3762, 0.3703,\n",
       "        0.3685, 0.3654, 0.3648, 0.3621, 0.3619, 0.3607, 0.3583, 0.3581, 0.3528,\n",
       "        0.3513, 0.3478, 0.3470, 0.3430, 0.3407, 0.3388, 0.3354, 0.3320, 0.3256,\n",
       "        0.3255, 0.3234, 0.3200, 0.3166, 0.3152, 0.3147, 0.3136, 0.3123, 0.3117,\n",
       "        0.3111, 0.3096, 0.3092, 0.3075, 0.3067, 0.3057, 0.3057, 0.3043, 0.3037,\n",
       "        0.3019, 0.3015, 0.3014, 0.3009, 0.2969, 0.2969, 0.2947, 0.2903, 0.2891,\n",
       "        0.2886, 0.2873, 0.2852, 0.2850, 0.2844, 0.2824, 0.2821, 0.2813, 0.2809,\n",
       "        0.2803], device='cuda:0', grad_fn=<TopkBackward0>),\n",
       "indices=tensor([ 4313,  2756, 49406,  3377,  2149, 14627,   786,  5922,  3941, 11024,\n",
       "         5829,  2677,  3551, 37578,  1237,  9518,  2782,  4682, 19338,  1791,\n",
       "         3912,  4055, 10469, 12608,  1574, 12679,   575,  4023,   286, 25989,\n",
       "        12375, 10157,  8512,  2533, 23278, 14426,  1452, 14069,   256,  1380,\n",
       "         1629,  2523, 31352,  9577,  2625, 31777,  1625, 44628, 15924,  2801,\n",
       "        44140, 13740,  2445, 31206,  2172, 14455,  2374, 36267, 13800, 25719,\n",
       "         5984,  1876,  2569,  1888,  5285, 39760, 10888,   269,  5035, 32946,\n",
       "          559,  6867,  2461,  3281,  5408, 23614, 23291, 30611,   607,  1901,\n",
       "         1212,  9975,  1952, 45559,  4612, 14304,  1746,  4899,  5579,  6267,\n",
       "         1312,  2218,  4369,  4966,  6885,  1137,  6135, 11179, 11692,  2568],\n",
       "       device='cuda:0'))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(sparse_encoding_im, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yard</w>\n",
      "garden</w>\n",
      "<|startoftext|>\n",
      "couple</w>\n",
      "guy</w>\n",
      "peoples</w>\n",
      "man</w>\n",
      "grass</w>\n",
      "walking</w>\n",
      "lawn</w>\n",
      "plants</w>\n",
      "tree</w>\n",
      "kid</w>\n",
      "bushes</w>\n",
      "two</w>\n",
      "brick</w>\n",
      "outside</w>\n",
      "trees</w>\n",
      "teenager</w>\n",
      "guys</w>\n"
     ]
    }
   ],
   "source": [
    "# after training\n",
    "for idx in torch.topk(sparse_encoding, 20).indices:\n",
    "    print(list(vocab.keys())[list(vocab.values()).index(idx)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yard</w> tensor(0.8103, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "garden</w> tensor(0.7990, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "<|startoftext|> tensor(0.7422, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "couple</w> tensor(0.7382, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "guy</w> tensor(0.7256, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "peoples</w> tensor(0.7129, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "man</w> tensor(0.7085, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "grass</w> tensor(0.6937, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "walking</w> tensor(0.6635, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "lawn</w> tensor(0.6454, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "plants</w> tensor(0.5775, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tree</w> tensor(0.5746, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "kid</w> tensor(0.5698, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "bushes</w> tensor(0.5514, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "two</w> tensor(0.5342, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "brick</w> tensor(0.5319, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "outside</w> tensor(0.5231, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "trees</w> tensor(0.5006, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "teenager</w> tensor(0.4979, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "guys</w> tensor(0.4936, device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# after training\n",
    "for idx in torch.topk(sparse_encoding, 20).indices:\n",
    "    print(list(vocab.keys())[list(vocab.values()).index(idx)], sparse_encoding[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.0004),\n",
       " tensor(0.6940),\n",
       " tensor(0.7278),\n",
       " tensor(0.7406),\n",
       " tensor(0.7426),\n",
       " tensor(0.7472),\n",
       " tensor(0.7556),\n",
       " tensor(0.7588),\n",
       " tensor(0.7598),\n",
       " tensor(0.7612),\n",
       " tensor(0.7588),\n",
       " tensor(0.7584),\n",
       " tensor(0.7588),\n",
       " tensor(0.7586),\n",
       " tensor(0.7590),\n",
       " tensor(0.7586)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#batch-size 64\n",
    "test_loss[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'epoch':epoch,\n",
    "    'model_state_dict': text_encoder.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': batch_loss,\n",
    "    }, f\"Models/\" + str(epoch) + \".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_encoding = text_encoder(test_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The man with pierced ears is wearing glasses and an orange hat.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_captions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|startoftext|> tensor(2.3467, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      ".</w> tensor(1.8225, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "orange</w> tensor(1.1488, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "man</w> tensor(1.1208, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "hat</w> tensor(1.0373, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "cap</w> tensor(0.8494, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "quarterback</w> tensor(0.8475, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "jersey</w> tensor(0.7866, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "hats</w> tensor(0.7030, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "red</w> tensor(0.7007, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "sunglasses</w> tensor(0.6857, device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# batch size 64 - thong pretrained models\n",
    "for idx in torch.topk(sparse_encoding, 11).indices:\n",
    "    print(list(vocab.keys())[list(vocab.values()).index(idx)], sparse_encoding[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orange</w> tensor(0.5841, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "hat</w> tensor(0.4481, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "glasses</w> tensor(0.4318, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "i</w> tensor(0.3357, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "cap</w> tensor(0.3126, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tattoos</w> tensor(0.2803, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "books</w> tensor(0.2756, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "shaped</w> tensor(0.2750, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "red</w> tensor(0.2650, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "headband</w> tensor(0.2535, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "made</w> tensor(0.2519, device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for idx in torch.topk(sparse_encoding, 11).indices:\n",
    "    print(list(vocab.keys())[list(vocab.values()).index(idx)], sparse_encoding[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|startoftext|> tensor(0.5583, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "orange</w> tensor(0.5373, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "glasses</w> tensor(0.4128, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "hat</w> tensor(0.3734, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "man</w> tensor(0.2442, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tattoos</w> tensor(0.2316, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "s tensor(0.2261, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tattoo</w> tensor(0.2177, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "hoodie</w> tensor(0.2038, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "shaped</w> tensor(0.2003, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "customers</w> tensor(0.1983, device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for idx in torch.topk(sparse_encoding, 11).indices:\n",
    "    print(list(vocab.keys())[list(vocab.values()).index(idx)], sparse_encoding[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cap</w> tensor(0.4192, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "man</w> tensor(0.3873, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "orange</w> tensor(0.3632, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "closely</w> tensor(0.3370, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "praying</w> tensor(0.3344, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "bush</w> tensor(0.3284, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "0</w> tensor(0.3167, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "beret</w> tensor(0.2977, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "themselves</w> tensor(0.2914, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "beanie</w> tensor(0.2838, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "marketplace</w> tensor(0.2830, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "cause</w> tensor(0.2813, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "horn</w> tensor(0.2793, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "stares</w> tensor(0.2779, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "scene</w> tensor(0.2775, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "distance</w> tensor(0.2648, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "glasses</w> tensor(0.2624, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "hat</w> tensor(0.2623, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "goggles</w> tensor(0.2612, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "eyes</w> tensor(0.2581, device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# old\n",
    "for idx in torch.topk(sparse_encoding, 20).indices:\n",
    "    print(list(vocab.keys())[list(vocab.values()).index(idx)], sparse_encoding[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(83.3571, device='cuda:0', grad_fn=<NormBackward1>)\n"
     ]
    }
   ],
   "source": [
    "plt.hist(sparse_encoding.cpu().detach().numpy(), bins=30)\n",
    "plt.show()\n",
    "\n",
    "l1_regularization = torch.norm(sparse_encoding, 1)\n",
    "print(l1_regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0000e+00, 3.5032e-44, 3.7835e-44,  ..., 3.6315e-01, 3.8732e-01,\n",
       "         4.1922e-01], device='cuda:0', grad_fn=<Unique2Backward0>),\n",
       " tensor([47637,     1,     1,  ...,     1,     1,     1], device='cuda:0'))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_encoding.unique(return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([False,  True], device='cuda:0'),\n",
       " tensor([49120,   288], device='cuda:0'))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sparse_encoding > 0.1).unique(return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datasets/flickr30k/test/1007129816.jpg'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_files[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
