{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import clip\n",
    "from PIL import Image\n",
    "import json\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open the json file for annotations\n",
    "f = open('datasets/flickr30k/dataset_flickr30k.json')\n",
    "data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps = [x['sentences'] for x in data['images'] if x.get('split') == 'train']\n",
    "\n",
    "files = []\n",
    "for x in data['images']:\n",
    "    if x.get('split') == 'train':\n",
    "        files.append('datasets/flickr30k/train/' + x['filename'])\n",
    "            \n",
    "captions = []\n",
    "for x in caps:\n",
    "    for y in x:\n",
    "        captions.append(y)\n",
    "    \n",
    "captions = [x.get('raw') for x in captions]\n",
    "\n",
    "# these captions are too long and need adjusting\n",
    "captions[13035] = 'Four young adults sit outside on a wooden deck near a building around a small round table, while another person stands on the edge of the deck, leaning on the wooden railing, with the sun shining on one of them, one holding a cellphone out in front of himself and another holding a green and red soda can.'\n",
    "captions[14580] = 'A man wearing a helmet, red pants with white and a white and red shirt is on a small bicycle using only his hands, while another man wearing a light blue shirt with dark blue trim and black pants with red stripes is standing nearby, gesturing toward the first man and holding a small figurine.'\n",
    "captions[120165] = 'In this photo there is a man in a dirty white shirt and a black hat with yellow stained teeth, he looks happy and it appears that he is also repairing something.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = files[0:1000]\n",
    "captions = captions [0:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "checkpoint = torch.load(\"D:/thesis/Models/flickr30k trained/81.pt\", map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 / 1000\r\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "i = 0\n",
    "for filename in files:\n",
    "    im=preprocess(Image.open(filename))\n",
    "    for j in range(5):\n",
    "        images.append(im)\n",
    "    i+=1\n",
    "    if (i%1000) == 0:\n",
    "        print(i,\"/\",len(files), end='\\r')\n",
    "        \n",
    "print(\"\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class image_caption_dataset():\n",
    "    def __init__(self, image_list, caption_list):\n",
    "\n",
    "        self.images = image_list\n",
    "        self.captions  = clip.tokenize(caption_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        caption = self.captions[idx]\n",
    "        return image,caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 64\n",
    "\n",
    "dataset = image_caption_dataset(images, captions)\n",
    "train_dataloader = DataLoader(dataset, batch_size = batchsize, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/openai/CLIP/issues/57\n",
    "def convert_models_to_fp32(model):\n",
    "    for p in model.parameters():\n",
    "        p.data = p.data.float()\n",
    "        p.grad.data = p.grad.data.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 1009962496h: 0 / 78\n",
      "2: 1009962496\n",
      "3: 3850907136\n",
      "4: 3850907136\n",
      "5: 1009962496\n",
      "6: 2226127360\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "loss_img = torch.nn.CrossEntropyLoss()\n",
    "loss_txt = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-7,betas=(0.9,0.98),eps=1e-6,weight_decay=0.001)\n",
    "\n",
    "# add your own code to track the training progress.\n",
    "epochs = 10\n",
    "totalbatches = int(len(dataset) / batchsize)\n",
    "for epoch in range(0, epochs):\n",
    "    i = 0\n",
    "    for batch in train_dataloader:\n",
    "        print(\"epoch:\", epoch, \"batch:\", i, \"/\", totalbatches, end='\\r')\n",
    "       \n",
    "        optimizer.zero_grad()\n",
    "        print(\"1:\", torch.cuda.memory_allocated())\n",
    "        ims, caps = batch \n",
    "        \n",
    "        ims= ims.to(device)\n",
    "        caps = caps.to(device)\n",
    "        print(\"2:\", torch.cuda.memory_allocated())\n",
    "        logits_per_image, logits_per_text = model(ims, caps)\n",
    "        print(\"3:\", torch.cuda.memory_allocated())\n",
    "        ground_truth = torch.arange(len(ims),dtype=torch.long,device=device)\n",
    "        print(\"4:\", torch.cuda.memory_allocated())\n",
    "        total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "        total_loss.backward()\n",
    "        print(\"5:\", torch.cuda.memory_allocated())\n",
    "        if device == \"cpu\":\n",
    "            optimizer.step()\n",
    "            i+=1\n",
    "        else : \n",
    "            convert_models_to_fp32(model)\n",
    "            optimizer.step()\n",
    "            clip.model.convert_weights(model)\n",
    "            i+=1\n",
    "        print(\"6:\", torch.cuda.memory_allocated())\n",
    "        break\n",
    "    break\n",
    "    torch.save({\n",
    "        'epoch':epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': total_loss,\n",
    "        }, f\"Models/\" + str(epoch) + \".pt\")\n",
    "    \n",
    "print(\"\")\n",
    "print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
