{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import glob\n",
    "import time\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\lib\\site-packages\\transformers\\models\\clip\\processing_clip.py:142: FutureWarning: `feature_extractor` is deprecated and will be removed in v5. Use `image_processor` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "preprocess = processor.feature_extractor\n",
    "tokenizer = processor.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "c:\\python\\lib\\site-packages\\transformers\\models\\clip\\processing_clip.py:142: FutureWarning: `feature_extractor` is deprecated and will be removed in v5. Use `image_processor` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "preprocess = processor.feature_extractor\n",
    "tokenizer = processor.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"Models/model_clip-vit-large-patch14/model.pt\", map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "c:\\python\\lib\\site-packages\\transformers\\models\\clip\\processing_clip.py:142: FutureWarning: `feature_extractor` is deprecated and will be removed in v5. Use `image_processor` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14-336\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14-336\")\n",
    "\n",
    "preprocess = processor.feature_extractor\n",
    "tokenizer = processor.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLM(nn.Module):\n",
    "    def __init__(self, dense_size, vocab_size):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(dense_size, vocab_size)\n",
    "        \n",
    "    def forward(self, dense_vec):\n",
    "        term_importances = torch.log1p(torch.relu(self.linear(dense_vec)))\n",
    "        return term_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sparse_performance(text_model, image_model, test_ims, test_cap):\n",
    "    with torch.no_grad():\n",
    "        x, _ = test_ims.shape\n",
    "        encoded_ims = torch.empty(int(x), 49408)\n",
    "        for i in range(0, len(test_ims), 64):\n",
    "            encoded_ims[i:i+64] = image_model(test_ims[i:i+64])\n",
    "\n",
    "\n",
    "        encoded_caps = torch.empty(int(x*5), 49408)\n",
    "        for i in range(0, len(encoded_caps), 64):\n",
    "            encoded_caps[i:i+64] = text_model(test_cap[i:i+64])   \n",
    "\n",
    "        encoded_ims = (encoded_ims / encoded_ims.norm(dim=-1, keepdim=True)).to(device)\n",
    "        encoded_caps = encoded_caps / encoded_caps.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        recall_1, recall_5, recall_10 = [],[],[]\n",
    "        i = 0\n",
    "        j = 0\n",
    "        image_id = 0\n",
    "        print(len(encoded_caps))\n",
    "        t = time.time()\n",
    "        for text_feature in encoded_caps:\n",
    "            if (j%500) == 0:\n",
    "                print(j, \", time since last: \", time.time() - t, end='\\r')\n",
    "                t = time.time()\n",
    "            similarity = (100.0 * text_feature.to(device) @ encoded_ims.T).softmax(dim=-1)\n",
    "            _, indices = similarity.topk(10)\n",
    "            \n",
    "            recall_1.append(image_id in indices[0])\n",
    "            recall_5.append(image_id in indices[:5])\n",
    "            recall_10.append(image_id in indices)\n",
    "\n",
    "            i += 1\n",
    "            j += 1\n",
    "            if i == 5:\n",
    "                i = 0\n",
    "                image_id += 1\n",
    "\n",
    "        print(\"\")\n",
    "        return torch.Tensor(recall_1).mean(), torch.Tensor(recall_5).mean(), torch.Tensor(recall_10).mean()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainBatches():\n",
    "    def __init__(self, image_vectors, captions):\n",
    "\n",
    "        self.images = image_vectors\n",
    "        self.captions  = captions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        caption = self.captions[idx]\n",
    "        return image,caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open the json file for annotations\n",
    "f = open('datasets/flickr30k/dataset_flickr30k.json')\n",
    "data = json.load(f)\n",
    "\n",
    "# load all captions\n",
    "caps = [x['sentences'] for x in data['images'] if x.get('split') == 'train']\n",
    "test_caps = [x['sentences'] for x in data['images'] if x.get('split') == 'test']\n",
    "\n",
    "files = []\n",
    "test_files = []\n",
    "for x in data['images']:\n",
    "    if x.get('split') == 'train':\n",
    "        files.append('datasets/flickr30k/train/' + x['filename'])\n",
    "    \n",
    "    elif x.get('split') == 'test':\n",
    "        test_files.append('datasets/flickr30k/test/' + x['filename'])\n",
    "            \n",
    "captions = []\n",
    "for x in caps:\n",
    "    for y in x:\n",
    "        captions.append(y)\n",
    "        \n",
    "test_captions = []\n",
    "for x in test_caps:\n",
    "    for y in x:\n",
    "        test_captions.append(y)\n",
    "    \n",
    "captions = [x.get('raw') for x in captions]\n",
    "test_captions = [x.get('raw') for x in test_captions]\n",
    "\n",
    "# these captions are too long and need adjusting\n",
    "captions[13035] = 'Four young adults sit outside on a wooden deck near a building around a small round table, while another person stands on the edge of the deck, leaning on the wooden railing, with the sun shining on one of them, one holding a cellphone out in front of himself and another holding a green and red soda can.'\n",
    "captions[14580] = 'A man wearing a helmet, red pants with white and a white and red shirt is on a small bicycle using only his hands, while another man wearing a light blue shirt with dark blue trim and black pants with red stripes is standing nearby, gesturing toward the first man and holding a small figurine.'\n",
    "captions[120165] = 'In this photo there is a man in a dirty white shirt and a black hat with yellow stained teeth, he looks happy and it appears that he is also repairing something.'\n",
    "test_captions[3905] = 'Two boys are looking upwards with their arms streteched to the sky, the boy on the left is wearing a blue vest jacket with a gray shirt, black jogging pants and a hat, and the boy on the right is wearing a silver vest jacket, with blue long-sleeved undershirt, gray pants, black tennis shoes and has black short hair and glasses.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = files[0:10]\n",
    "captions = captions[0:50]\n",
    "\n",
    "test_files = test_files[0:10]\n",
    "test_captions = test_captions[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28900 / 29000 / 29000 / 29000 / 29000 / 29000 / 29000 / 29000 / 29000 / 29000 / 29000 / 29000 / 29000 / 29000\n",
      "done\n",
      "293.0039150714874\n"
     ]
    }
   ],
   "source": [
    "# encode images\n",
    "L = len(files)\n",
    "batch_size = 64\n",
    "t1 = time.time()\n",
    "with torch.no_grad():\n",
    "    encoded_ims = torch.Tensor()\n",
    "    for i in range(0, L, batch_size):\n",
    "        print(i,\"/\",L, end='\\r')\n",
    "        images = torch.Tensor().to(device)\n",
    "        for x in range(batch_size):\n",
    "            if (i + x) < L:\n",
    "                image = preprocess(Image.open(files[i+x]), return_tensors='pt')['pixel_values'].to(device)\n",
    "                images = torch.cat((images, image), 0)\n",
    "                \n",
    "        ims = model.vision_model(images).pooler_output\n",
    "        ims = model.visual_projection(ims)\n",
    "        encoded_ims = torch.cat((encoded_ims, ims.to(\"cpu\")), 0)\n",
    "\n",
    "p = 0\n",
    "encoded_images = torch.Tensor().to(device)\n",
    "for image in encoded_ims:\n",
    "    if (p%100) == 0:\n",
    "        print(p,\"/\",L, end='\\r')\n",
    "        \n",
    "    encoded_images = torch.cat((encoded_images, image.to(device).repeat(5,1)), 0)\n",
    "    p += 1\n",
    "    \n",
    "print(\"\")\n",
    "print(\"done\")\n",
    "print(time.time() - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vitb32, vitL14@336, vitL14, large14-thong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_images = torch.load(\"pt datafiles/flickr/MLP/encoded_images_large14-thong.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([145000, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoded_images, \"encoded_images_vitb32.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144896 / 145000145000 / 145000 / 145000145000 / 145000 / 145000 / 145000145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000145000 / 145000 / 145000 / 145000 / 145000 / 145000145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000 / 145000\n",
      "done\n",
      "282.8192572593689\n"
     ]
    }
   ],
   "source": [
    "# encode captions\n",
    "L = len(captions)\n",
    "batch_size = 128\n",
    "t1 = time.time()\n",
    "with torch.no_grad():\n",
    "    text_features = torch.Tensor().to(device)\n",
    "    for i in range(0, L, batch_size):\n",
    "        print(i,\"/\",L, end='\\r')\n",
    "        text = tokenizer(captions[i:i+batch_size], padding=True, return_tensors='pt').to(device)\n",
    "        \n",
    "        text_output = model.text_model(**text).pooler_output\n",
    "        text_output = model.text_projection(text_output)      \n",
    "        text_features = torch.cat((text_features, text_output), 0)\n",
    "        \n",
    "print(\"\")\n",
    "print(\"done\")\n",
    "print(time.time() - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features = torch.load(\"pt datafiles/flickr/MLP/encoded_captions_large14-thong.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2601608192\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(text_features, \"encoded_captions_large14-thong.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "992 / 1000 2621942784\n",
      "done\n",
      "53.622819900512695\n"
     ]
    }
   ],
   "source": [
    "# encode images\n",
    "L = len(test_files)\n",
    "batch_size = 32\n",
    "t1 = time.time()\n",
    "with torch.no_grad():\n",
    "    test_ims = torch.Tensor()\n",
    "    for i in range(0, L, batch_size):\n",
    "        print(i,\"/\",L, torch.cuda.memory_allocated(), end='\\r')\n",
    "        images = torch.Tensor().to(device)\n",
    "        for x in range(batch_size):\n",
    "            if (i + x) < L:\n",
    "                image = preprocess(Image.open(test_files[i+x]), return_tensors='pt')['pixel_values'].to(device)\n",
    "                images = torch.cat((images, image), 0)\n",
    "                \n",
    "        ims = model.vision_model(images).pooler_output\n",
    "        ims = model.visual_projection(ims)\n",
    "        test_ims = torch.cat((test_ims, ims.to(\"cpu\")), 0)\n",
    "\n",
    "test_images = test_ims.to(device)\n",
    "print(\"\")\n",
    "print(\"done\")\n",
    "print(time.time() - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = torch.load(\"pt datafiles/flickr/MLP/test_images_large14-thong.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2604680192\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(test_images, \"test_images_large14-thong.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4992 / 5000/ 5000 / 5000 / 5000 / 5000\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# encode test captions\n",
    "L = len(test_captions)\n",
    "batch_size = 128\n",
    "with torch.no_grad():\n",
    "    test_features = torch.Tensor().to(device)\n",
    "    for i in range(0, L, batch_size):\n",
    "        print(i,\"/\",L, end='\\r')\n",
    "        text = tokenizer(test_captions[i:i+batch_size], padding=True, return_tensors='pt').to(device)\n",
    "        \n",
    "        text_output = model.text_model(**text).pooler_output\n",
    "        text_output = model.text_projection(text_output)      \n",
    "        test_features = torch.cat((test_features, text_output), 0)\n",
    "        \n",
    "print(\"\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = torch.load(\"pt datafiles/flickr/MLP/test_captions_large14-thong.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(test_features, \"test_captions_large14-thong.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 256\n",
    "\n",
    "dataset = TrainBatches(encoded_images, text_features)\n",
    "train_dataloader = DataLoader(dataset, batch_size = batchsize, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_text_size = model.text_projection.weight.shape[0]\n",
    "dense_image_size = model.visual_projection.weight.shape[0]\n",
    "vocab_size = model.text_model.config.vocab_size\n",
    "\n",
    "text_encoder = MLM(dense_text_size, vocab_size).to(device)\n",
    "image_encoder = MLM(dense_image_size, vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_checkpoint = torch.load(\"Models/MLM8-336.pt\", map_location='cpu')\n",
    "\n",
    "image_encoder = MLM(dense_image_size, vocab_size).to(device)\n",
    "image_encoder.load_state_dict(im_checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_checkpoint = torch.load(\"Models/model_clip-vit-large-patch14/image_encoder.pt\", map_location='cpu')\n",
    "\n",
    "image_encoder = MLM(dense_image_size, vocab_size).to(device)\n",
    "image_encoder.load_state_dict(im_checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "4500 , time since last:  0.48864364624023443\n",
      "tensor(0.0024)\n",
      "tensor(0.0042)\n",
      "tensor(0.0096)\n"
     ]
    }
   ],
   "source": [
    "#before training\n",
    "rec1,rec5,rec10 = test_sparse_performance(text_encoder, image_encoder, test_images, test_features)\n",
    "\n",
    "print(rec1)\n",
    "print(rec5)\n",
    "print(rec10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 batch: 550 / 566\n",
      "loss: 555.7643805444241\n",
      "testing recall scores..\n",
      "5000\n",
      "4500 , time since last:  0.48485398292541504\n",
      "R@1: tensor(0.6862)\n",
      "epoch done\n",
      "\n",
      "epoch: 1 batch: 250 / 566\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\LALASH~1\\AppData\\Local\\Temp/ipykernel_13000/834697901.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mloss_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits_per_image\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mground_truth\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss_txt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits_per_text\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mground_truth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mbatch_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_img = torch.nn.CrossEntropyLoss()\n",
    "loss_txt = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(text_encoder.parameters(), lr=1e-4,betas=(0.9,0.98),eps=1e-6,weight_decay=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[5,10], gamma=0.1)\n",
    "\n",
    "vocab_size = model.text_model.config.vocab_size\n",
    "epochs = 15\n",
    "totalbatches = int(len(dataset) / batchsize)\n",
    "logit_scale = model.logit_scale.exp().item()\n",
    "losses=[]\n",
    "test_loss=[[rec1],[rec5],[rec10]]\n",
    "for epoch in range(0, epochs):\n",
    "    i = 0\n",
    "    batch_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        if i % 25 == 0:\n",
    "            print(\"epoch:\", epoch, \"batch:\", i, \"/\", totalbatches, end='\\r')\n",
    "       \n",
    "        optimizer.zero_grad()\n",
    "        dense_images, dense_caps = batch\n",
    "        \n",
    "        # sparse encoding\n",
    "        with torch.no_grad():\n",
    "            sparse_ims = image_encoder(dense_images)\n",
    "            \n",
    "        sparse_caps = text_encoder(dense_caps)\n",
    "\n",
    "        # determine logits\n",
    "        sparse_ims = sparse_ims / sparse_ims.norm(dim=-1, keepdim=True)\n",
    "        sparse_caps = sparse_caps / (sparse_caps + 1e-20).norm(dim=-1, keepdim=True)\n",
    "        logits_per_image = logit_scale * sparse_ims @ sparse_caps.t()\n",
    "        logits_per_text = logits_per_image.t()\n",
    "        \n",
    "        # compute losses\n",
    "        ground_truth = torch.arange(len(dense_images),dtype=torch.long,device=device)\n",
    "        \n",
    "        loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "            \n",
    "        batch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        i+=1\n",
    "        \n",
    "    scheduler.step()\n",
    "    losses.append(batch_loss)\n",
    "    print(\"\")\n",
    "    print(\"loss:\", batch_loss)\n",
    "    \n",
    "    print(\"testing recall scores..\")\n",
    "    recall1,recall5,recall10 = test_sparse_performance(text_encoder, image_encoder, test_images, test_features)\n",
    "    test_loss[0].append(recall1)\n",
    "    test_loss[1].append(recall5)\n",
    "    test_loss[2].append(recall10)\n",
    "    print(\"R@1:\", recall1)\n",
    "    print(\"epoch done\")\n",
    "    print(\"\")\n",
    "\"\"\"\n",
    "    torch.save({\n",
    "        'epoch':epoch,\n",
    "        'model_state_dict': sparse_mlm.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': total_loss,\n",
    "        }, f\"Models/\" + str(epoch) + \".pt\")\n",
    "\"\"\"      \n",
    "print(\"\")\n",
    "print(\"done\")\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.title('CLIP loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor(0.0008),\n",
       "  tensor(0.6878),\n",
       "  tensor(0.7210),\n",
       "  tensor(0.7274),\n",
       "  tensor(0.7376),\n",
       "  tensor(0.7410),\n",
       "  tensor(0.7524),\n",
       "  tensor(0.7524),\n",
       "  tensor(0.7546),\n",
       "  tensor(0.7584),\n",
       "  tensor(0.7582),\n",
       "  tensor(0.7594),\n",
       "  tensor(0.7598),\n",
       "  tensor(0.7592),\n",
       "  tensor(0.7596),\n",
       "  tensor(0.7594)],\n",
       " [tensor(0.0034),\n",
       "  tensor(0.9198),\n",
       "  tensor(0.9302),\n",
       "  tensor(0.9360),\n",
       "  tensor(0.9366),\n",
       "  tensor(0.9372),\n",
       "  tensor(0.9398),\n",
       "  tensor(0.9414),\n",
       "  tensor(0.9422),\n",
       "  tensor(0.9422),\n",
       "  tensor(0.9428),\n",
       "  tensor(0.9434),\n",
       "  tensor(0.9436),\n",
       "  tensor(0.9426),\n",
       "  tensor(0.9434),\n",
       "  tensor(0.9434)],\n",
       " [tensor(0.0078),\n",
       "  tensor(0.9596),\n",
       "  tensor(0.9662),\n",
       "  tensor(0.9680),\n",
       "  tensor(0.9692),\n",
       "  tensor(0.9716),\n",
       "  tensor(0.9714),\n",
       "  tensor(0.9722),\n",
       "  tensor(0.9722),\n",
       "  tensor(0.9722),\n",
       "  tensor(0.9714),\n",
       "  tensor(0.9714),\n",
       "  tensor(0.9712),\n",
       "  tensor(0.9714),\n",
       "  tensor(0.9712),\n",
       "  tensor(0.9716)]]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "tensor(0.9906, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    total_nonzero = 0\n",
    "    i = 0\n",
    "    for feature in text_features:\n",
    "        sparse = text_encoder(feature)\n",
    "        total_nonzero += (len(sparse) - torch.count_nonzero(sparse))/len(sparse)\n",
    "        i += 1\n",
    "        if (i%10000) == 0:\n",
    "            print(i)\n",
    "        \n",
    "    print(total_nonzero/i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(test_loss[0][1:])\n",
    "plt.title('CLIP loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open the json file for annotations\n",
    "f = open(\"C:/Users/Lalashops/Desktop/MasterThesis/random images/vocab.json\", encoding = 'utf-8')\n",
    "vocab = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_encoding = text_encoder(test_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The man with pierced ears is wearing glasses and an orange hat.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_captions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hat</w>\n",
      "cap</w>\n",
      "glasses</w>\n",
      "man</w>\n",
      "anchor</w>\n",
      "orange</w>\n",
      "chopped</w>\n",
      "<|startoftext|>\n",
      "scarf</w>\n",
      "inste\n"
     ]
    }
   ],
   "source": [
    "# after training\n",
    "for idx in torch.topk(sparse_encoding, 10).indices:\n",
    "    print(list(vocab.keys())[list(vocab.values()).index(idx)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_encoding = text_encoder(test_features[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A black and white dog is running in a grassy garden surrounded by a white fence.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_captions[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caption:\n"
     ]
    }
   ],
   "source": [
    "print(\"caption:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 output terms:\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 10 output terms:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. fence</w>\n",
      "2. wristband</w>\n",
      "3. dog</w>\n",
      "4. white</w>\n",
      "5. black</w>\n",
      "6. process</w>\n",
      "7. runs</w>\n",
      "8. flower</w>\n",
      "9. lawn</w>\n",
      "10. outdoor</w>\n"
     ]
    }
   ],
   "source": [
    "# after training\n",
    "i = 1\n",
    "for idx in torch.topk(sparse_encoding, 10).indices:\n",
    "    print(str(i) + \". \" +list(vocab.keys())[list(vocab.values()).index(idx)])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.0004),\n",
       " tensor(0.6940),\n",
       " tensor(0.7278),\n",
       " tensor(0.7406),\n",
       " tensor(0.7426),\n",
       " tensor(0.7472),\n",
       " tensor(0.7556),\n",
       " tensor(0.7588),\n",
       " tensor(0.7598),\n",
       " tensor(0.7612),\n",
       " tensor(0.7588),\n",
       " tensor(0.7584),\n",
       " tensor(0.7588),\n",
       " tensor(0.7586),\n",
       " tensor(0.7590),\n",
       " tensor(0.7586)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#batch-size 64\n",
    "test_loss[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'epoch':epoch,\n",
    "    'model_state_dict': text_encoder.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': batch_loss,\n",
    "    }, f\"Models/\" + str(epoch) + \".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_encoding = text_encoder(test_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The man with pierced ears is wearing glasses and an orange hat.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_captions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|startoftext|> tensor(2.3467, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      ".</w> tensor(1.8225, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "orange</w> tensor(1.1488, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "man</w> tensor(1.1208, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "hat</w> tensor(1.0373, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "cap</w> tensor(0.8494, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "quarterback</w> tensor(0.8475, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "jersey</w> tensor(0.7866, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "hats</w> tensor(0.7030, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "red</w> tensor(0.7007, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "sunglasses</w> tensor(0.6857, device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# batch size 64 - thong pretrained models\n",
    "for idx in torch.topk(sparse_encoding, 11).indices:\n",
    "    print(list(vocab.keys())[list(vocab.values()).index(idx)], sparse_encoding[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orange</w> tensor(0.5841, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "hat</w> tensor(0.4481, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "glasses</w> tensor(0.4318, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "i</w> tensor(0.3357, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "cap</w> tensor(0.3126, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tattoos</w> tensor(0.2803, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "books</w> tensor(0.2756, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "shaped</w> tensor(0.2750, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "red</w> tensor(0.2650, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "headband</w> tensor(0.2535, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "made</w> tensor(0.2519, device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for idx in torch.topk(sparse_encoding, 11).indices:\n",
    "    print(list(vocab.keys())[list(vocab.values()).index(idx)], sparse_encoding[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|startoftext|> tensor(0.5583, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "orange</w> tensor(0.5373, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "glasses</w> tensor(0.4128, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "hat</w> tensor(0.3734, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "man</w> tensor(0.2442, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tattoos</w> tensor(0.2316, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "s tensor(0.2261, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tattoo</w> tensor(0.2177, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "hoodie</w> tensor(0.2038, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "shaped</w> tensor(0.2003, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "customers</w> tensor(0.1983, device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for idx in torch.topk(sparse_encoding, 11).indices:\n",
    "    print(list(vocab.keys())[list(vocab.values()).index(idx)], sparse_encoding[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cap</w> tensor(0.4192, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "man</w> tensor(0.3873, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "orange</w> tensor(0.3632, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "closely</w> tensor(0.3370, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "praying</w> tensor(0.3344, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "bush</w> tensor(0.3284, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "0</w> tensor(0.3167, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "beret</w> tensor(0.2977, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "themselves</w> tensor(0.2914, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "beanie</w> tensor(0.2838, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "marketplace</w> tensor(0.2830, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "cause</w> tensor(0.2813, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "horn</w> tensor(0.2793, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "stares</w> tensor(0.2779, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "scene</w> tensor(0.2775, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "distance</w> tensor(0.2648, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "glasses</w> tensor(0.2624, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "hat</w> tensor(0.2623, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "goggles</w> tensor(0.2612, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "eyes</w> tensor(0.2581, device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# old\n",
    "for idx in torch.topk(sparse_encoding, 20).indices:\n",
    "    print(list(vocab.keys())[list(vocab.values()).index(idx)], sparse_encoding[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD5CAYAAADFqlkBAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAPz0lEQVR4nO3df6zddX3H8efLVpSpCMJdQ9q6y2aTpZrNH3fQxWWZskGBhZIMCWQbnWlsNiBz0WTWzYQMJIGZyCRBZzMaW+NWOjdDo8WuAYzxj2JvFcHCCFeE0AZttbVoiJjie3/cT91Zvfeeb6H3nLt7n4/k5H6+7+/n+z2f80m5r3vO93O+pKqQJC1srxj2ACRJw2cYSJIMA0mSYSBJwjCQJGEYSJKAxV06JXkK+DHwInCsqsaSvAG4GxgFngKuqqojSQJ8ArgUeB74i6r6RjvPWuAj7bQfrarNrf4O4DPA6cAO4P3VZ83rOeecU6Ojo11fpyQteHv37v1BVY1Mta9TGDTvqqof9GxvAO6rqluTbGjbHwIuAVa0xwXAp4ALWnjcCIwBBexNsr2qjrQ+7wMeZDIMVgP3zjSY0dFRxsfHT2L4krSwJXl6un0v52OiNcDm1t4MXNFT31KTdgNnJjkXuBjYVVWHWwDsAla3fWdU1e72bmBLz7kkSQPQNQwK+K8ke5Osb7UlVfVsa38PWNLaS4Fneo7d32oz1fdPUZckDUjXj4l+r6oOJPlVYFeS/+7dWVWVZNbva9GCaD3AG9/4xtl+OklaMDq9M6iqA+3nQeALwPnA99tHPLSfB1v3A8DynsOXtdpM9WVT1Kcax8aqGquqsZGRKa+BSJJegr5hkOQ1SV53vA1cBHwb2A6sbd3WAve09nbg2kxaBRxtHyftBC5KclaSs9p5drZ9zyVZ1VYiXdtzLknSAHT5mGgJ8IXJ39MsBv61qr6cZA+wLck64GngqtZ/B5PLSieYXFr6XoCqOpzkZmBP63dTVR1u7ev436Wl99JnJZEk6dTK/9dbWI+NjZVLSyWpuyR7q2psqn1+A1mSZBhIkk7uG8jzxuiGL3Xq99Stl83ySCRpbvCdgSTJMJAkGQaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgSeIkwiDJoiTfTPLFtn1ekgeTTCS5O8lprf6qtj3R9o/2nOPDrf54kot76qtbbSLJhlP4+iRJHZzMO4P3A4/1bN8G3F5VbwKOAOtafR1wpNVvb/1IshK4GngzsBr4ZAuYRcCdwCXASuCa1leSNCCdwiDJMuAy4F/adoB3A59vXTYDV7T2mrZN239h678G2FpVL1TVd4EJ4Pz2mKiqJ6vqZ8DW1leSNCBd3xn8E/C3wM/b9tnAj6rqWNveDyxt7aXAMwBt/9HW/xf1E46Zrv5LkqxPMp5k/NChQx2HLknqp28YJPlj4GBV7R3AeGZUVRuraqyqxkZGRoY9HEmaNxZ36PNO4PIklwKvBs4APgGcmWRx++t/GXCg9T8ALAf2J1kMvB74YU/9uN5jpqtLkgag7zuDqvpwVS2rqlEmLwDfX1V/CjwAXNm6rQXuae3tbZu2//6qqla/uq02Og9YAXwd2AOsaKuTTmvPsf2UvDpJUidd3hlM50PA1iQfBb4J3NXqdwGfTTIBHGbylztVtS/JNuBR4BhwfVW9CJDkBmAnsAjYVFX7Xsa4JEkn6aTCoKq+AnyltZ9kciXQiX1+CrxnmuNvAW6Zor4D2HEyY5EknTp+A1mSZBhIkgwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQ6hEGSVyf5epJvJdmX5B9a/bwkDyaZSHJ3ktNa/VVte6LtH+0514db/fEkF/fUV7faRJINs/A6JUkz6PLO4AXg3VX128BbgdVJVgG3AbdX1ZuAI8C61n8dcKTVb2/9SLISuBp4M7Aa+GSSRUkWAXcClwArgWtaX0nSgPQNg5r0k7b5yvYo4N3A51t9M3BFa69p27T9FyZJq2+tqheq6rvABHB+e0xU1ZNV9TNga+srSRqQTtcM2l/wDwEHgV3Ad4AfVdWx1mU/sLS1lwLPALT9R4Gze+snHDNdfapxrE8ynmT80KFDXYYuSeqgUxhU1YtV9VZgGZN/yf/mbA5qhnFsrKqxqhobGRkZxhAkaV46qdVEVfUj4AHgd4Ezkyxuu5YBB1r7ALAcoO1/PfDD3voJx0xXlyQNSJfVRCNJzmzt04E/Ah5jMhSubN3WAve09va2Tdt/f1VVq1/dVhudB6wAvg7sAVa01UmnMXmRefspeG2SpI4W9+/CucDmturnFcC2qvpikkeBrUk+CnwTuKv1vwv4bJIJ4DCTv9ypqn1JtgGPAseA66vqRYAkNwA7gUXApqrad8peoSSpr75hUFUPA2+bov4kk9cPTqz/FHjPNOe6BbhlivoOYEeH8UqSZoHfQJYkGQaSJMNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJIkOYZBkeZIHkjyaZF+S97f6G5LsSvJE+3lWqyfJHUkmkjyc5O0951rb+j+RZG1P/R1JHmnH3JEks/FiJUlT6/LO4BjwwapaCawCrk+yEtgA3FdVK4D72jbAJcCK9lgPfAomwwO4EbgAOB+48XiAtD7v6zlu9ct/aZKkrvqGQVU9W1XfaO0fA48BS4E1wObWbTNwRWuvAbbUpN3AmUnOBS4GdlXV4ao6AuwCVrd9Z1TV7qoqYEvPuSRJA3BS1wySjAJvAx4EllTVs23X94Alrb0UeKbnsP2tNlN9/xT1qZ5/fZLxJOOHDh06maFLkmbQOQySvBb4D+Bvquq53n3tL/o6xWP7JVW1sarGqmpsZGRktp9OkhaMTmGQ5JVMBsHnquo/W/n77SMe2s+DrX4AWN5z+LJWm6m+bIq6JGlAuqwmCnAX8FhVfbxn13bg+IqgtcA9PfVr26qiVcDR9nHSTuCiJGe1C8cXATvbvueSrGrPdW3PuSRJA7C4Q593An8OPJLkoVb7O+BWYFuSdcDTwFVt3w7gUmACeB54L0BVHU5yM7Cn9bupqg639nXAZ4DTgXvbQ5I0IH3DoKq+Bky37v/CKfoXcP0059oEbJqiPg68pd9YJEmzw28gS5IMA0mSYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CSRIcwSLIpycEk3+6pvSHJriRPtJ9ntXqS3JFkIsnDSd7ec8za1v+JJGt76u9I8kg75o4kOdUvUpI0sy7vDD4DrD6htgG4r6pWAPe1bYBLgBXtsR74FEyGB3AjcAFwPnDj8QBpfd7Xc9yJzyVJmmV9w6CqvgocPqG8Btjc2puBK3rqW2rSbuDMJOcCFwO7qupwVR0BdgGr274zqmp3VRWwpedckqQBeanXDJZU1bOt/T1gSWsvBZ7p6be/1Waq75+iPqUk65OMJxk/dOjQSxy6JOlEL/sCcvuLvk7BWLo818aqGquqsZGRkUE8pSQtCC81DL7fPuKh/TzY6geA5T39lrXaTPVlU9QlSQP0UsNgO3B8RdBa4J6e+rVtVdEq4Gj7OGkncFGSs9qF44uAnW3fc0lWtVVE1/acS5I0IIv7dUjyb8AfAOck2c/kqqBbgW1J1gFPA1e17juAS4EJ4HngvQBVdTjJzcCe1u+mqjp+Ufo6JlcsnQ7c2x6SpAHqGwZVdc00uy6com8B109znk3Apinq48Bb+o1DkjR7/AayJMkwkCQZBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJKYQ2GQZHWSx5NMJNkw7PFI0kIyJ8IgySLgTuASYCVwTZKVwx2VJC0ccyIMgPOBiap6sqp+BmwF1gx5TJK0YCwe9gCapcAzPdv7gQuGNJZfGN3wpWEPoa+nbr1s2EOQNA/MlTDoJMl6YH3b/EmSx1/iqc4BfnBqRjVcuW1WTjtv5mcWOUf9OUczG8b8/Np0O+ZKGBwAlvdsL2u1/6OqNgIbX+6TJRmvqrGXe575yvnpzznqzzma2Vybn7lyzWAPsCLJeUlOA64Gtg95TJK0YMyJdwZVdSzJDcBOYBGwqar2DXlYkrRgzIkwAKiqHcCOAT3dy/6oaZ5zfvpzjvpzjmY2p+YnVTXsMUiShmyuXDOQJA3RvA6Dfre4SPKqJHe3/Q8mGR3CMIemw/z8fpJvJDmW5MphjHHYOszRB5I8muThJPclmXbp3nzUYX7+MskjSR5K8rWFeGeBrrfaSfInSSrJcFYYVdW8fDB5Ifo7wK8DpwHfAlae0Oc64J9b+2rg7mGPe47NzyjwW8AW4Mphj3mOztG7gF9p7b/y39Avzc8ZPe3LgS8Pe9xzbY5av9cBXwV2A2PDGOt8fmfQ5RYXa4DNrf154MIkGeAYh6nv/FTVU1X1MPDzYQxwDugyRw9U1fNtczeT35FZKLrMz3M9m68BFtpFyq632rkZuA346SAH12s+h8FUt7hYOl2fqjoGHAXOHsjohq/L/Cx0JztH64B7Z3VEc0un+UlyfZLvAP8I/PWAxjZX9J2jJG8HllfVUO9/M5/DQBqYJH8GjAEfG/ZY5pqqurOqfgP4EPCRYY9nLknyCuDjwAeHPZb5HAZdbnHxiz5JFgOvB344kNENX6dbgCxwneYoyR8Cfw9cXlUvDGhsc8HJ/hvaClwxmwOag/rN0euAtwBfSfIUsArYPoyLyPM5DLrc4mI7sLa1rwTur3Y1ZwHwFiD99Z2jJG8DPs1kEBwcwhiHqcv8rOjZvAx4YoDjmwtmnKOqOlpV51TVaFWNMnnd6fKqGh/0QOdtGLRrAMdvcfEYsK2q9iW5KcnlrdtdwNlJJoAPAAvm/7DWZX6S/E6S/cB7gE8nWVC3COn4b+hjwGuBf2/LJxdMoHacnxuS7EvyEJP/ja2d+mzzU8c5mhP8BrIkaf6+M5AkdWcYSJIMA0mSYSBJwjCQJGEYSJIwDCRJGAaSJOB/AG5x1bVvqxOyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(83.3571, device='cuda:0', grad_fn=<NormBackward1>)\n"
     ]
    }
   ],
   "source": [
    "plt.hist(sparse_encoding.cpu().detach().numpy(), bins=30)\n",
    "plt.show()\n",
    "\n",
    "l1_regularization = torch.norm(sparse_encoding, 1)\n",
    "print(l1_regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0000e+00, 3.5032e-44, 3.7835e-44,  ..., 3.6315e-01, 3.8732e-01,\n",
       "         4.1922e-01], device='cuda:0', grad_fn=<Unique2Backward0>),\n",
       " tensor([47637,     1,     1,  ...,     1,     1,     1], device='cuda:0'))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_encoding.unique(return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([False,  True], device='cuda:0'),\n",
       " tensor([49120,   288], device='cuda:0'))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sparse_encoding > 0.1).unique(return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datasets/flickr30k/test/1007129816.jpg'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_files[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
